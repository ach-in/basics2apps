{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to PyTorch DataLoader\n",
    "\n",
    "In this lab, we're going to take a look at PyTorch's dataloader class and how it can be used to load your data in minibatches. We'll see how you can implement your custom Dataset and wrap it using pytorch's dataloader for efficient training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In PyTorch loading your custom dataset is easy, just inherit the PyTorch's Dataset class and implement the \n",
    "# __getitem__ and __len__ methods.\n",
    "\n",
    "# Here we are going to write a custom dataset for LAB-1 numpy data.\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, numpy_train_data, numpy_train_labels):\n",
    "        '''\n",
    "        numpy_train_data: path to training data numpy file\n",
    "        numpy_train_labels: path to training labels numpy file\n",
    "        '''\n",
    "        \n",
    "        self.train_data = np.load(numpy_train_data)\n",
    "        self.train_labels = np.load(numpy_train_labels)\n",
    "        \n",
    "    def __getitem__(self, idx): \n",
    "        '''\n",
    "        idx: index of the data point to return\n",
    "        '''\n",
    "        # here we are returning the single data point and label at position idx\n",
    "        return self.train_data[idx], self.train_labels[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        # here we return the length of the dataset\n",
    "        return self.train_data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the norb data to current folder or do a soft-link. And run the next command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "custom_data = CustomDataset('norb/train.npy', 'norb/train_cat.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has shape: (29160, 1, 108, 108)\n",
      "labels has shape: (29160,)\n"
     ]
    }
   ],
   "source": [
    "# To retrieve a single sample, just index the object.\n",
    "idx = 10\n",
    "\n",
    "data, labels = custom_data[:]\n",
    "print(\"data has shape: {}\".format(data.shape))\n",
    "print(\"labels has shape: {}\".format(labels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we can wrap our custom dataset using PyTorch's dataloader, which has many functionalities inbuilt\n",
    "# to ease the process of setting up training.\n",
    "\n",
    "# DataLoader can be used to wrap any Dataset and functions as a iterator to load data samples.\n",
    "\n",
    "# We can define our batch size to be loaded, wheshufflether to shuffle the batch,\n",
    "# and lastly number of parallel workers for loading the data batch.\n",
    "\n",
    "dataloader = DataLoader(custom_data, batch_size = 4, shuffle = True, num_workers = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader data_batch has shape: torch.Size([4, 1, 108, 108])\n",
      "Dataloader labels_batch has shape: torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "# Now we can load the data by iterating over the dataloader itself to retrieve batches of data.\n",
    "\n",
    "# We set our batch size to be 4 in the previous step, so in a single iteration we should be retrieving 4 samples at \n",
    "# once\n",
    "\n",
    "for data_batch, labels_batch in dataloader:\n",
    "    print(\"DataLoader data_batch has shape: {}\".format(data_batch.size()))\n",
    "    print(\"Dataloader labels_batch has shape: {}\".format(labels_batch.size()))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the DataLoader and Dataset class in PyTorch can be used to easily create custom loaders and load the data in batches with ease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
