{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from model import create_fcn as create_fcn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "The data has been arranged in train and test directories. train.npy and test.npy contain the input images for the test and train sets. Likewise, train_cat.npy and test_cat.npy contain the corresponding labels.\n",
    "We first load the data using `np.load()` and check out the shapes of numpy arrays using `np.shape`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29160, 1, 108, 108) (29160,)\n",
      "(29160, 1, 108, 108) (29160,)\n",
      "<class 'numpy.ndarray'>\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "train_data = np.load('norb/train.npy')\n",
    "train_labels = np.load('norb/train_cat.npy')\n",
    "\n",
    "test_data = np.load('norb/test.npy')\n",
    "test_labels = np.load('norb/test_cat.npy')\n",
    "\n",
    "# Let's print the shapes of these numpy arrays\n",
    "print(train_data.shape, train_labels.shape)\n",
    "print(test_data.shape, test_labels.shape)\n",
    "\n",
    "\n",
    "# Let's also check the data type of these variables\n",
    "print(type(train_data))\n",
    "for i in range(10):\n",
    "    print(train_labels[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the test data is quite large in size. We'll deal with a small subset of the test set for validation and use the rest for testing later. For this, we'll have to slice the test set in its first dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1, 108, 108) (1000,)\n"
     ]
    }
   ],
   "source": [
    "# Choosing a subset (first 1000) of the test set for validation purposes.\n",
    "test_data = test_data[:1000]\n",
    "test_labels = test_labels[:1000]\n",
    "\n",
    "# Let's verify\n",
    "print(test_data.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting our data from numpy arrays to PyTorch tensors.\n",
    "So far, we've been working with numpy arrays. For performing further operations, like forward prop, accessing cuda, we should convert the numpy arrays into PyTorch tensors. It's simple: use `torch.from_numpy()` for this. Typecasting can be accomplished by simply calling the corresponding functions: `x.float()` or `x.long()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# Converting to PyTorch tensors\n",
    "train_data = torch.from_numpy(train_data).float()\n",
    "test_data = torch.from_numpy(test_data).float()\n",
    "\n",
    "# If we're planning to use cross entropy loss, the data type of the\n",
    "# targets needs to be 'long'.\n",
    "train_labels = torch.from_numpy(train_labels).long()\n",
    "test_labels = torch.from_numpy(test_labels).long()\n",
    "\n",
    "print(type(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([29160, 1, 108, 108])\n",
      "29160\n"
     ]
    }
   ],
   "source": [
    "# When dealing with PyTorch tensors, it is recommended to use x.size()\n",
    "# instead of x.shape to find the shape/size of the tensor\n",
    "print(train_data.size())\n",
    "print(train_data.size(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting into Cuda tensors\n",
    "Since we're going to use GPUs, the variables first need to be converted to cuda-type. For doing this, use `x = x.cuda()`. This, in effect, loads the tensors into your GPUs memory. This operation should be used very judiciously because if mishandled the data transfer itself could introduce major time delays. More on this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "# Convert the data and labels into cuda Variables now: x = x.cuda()\n",
    "train_data = train_data.cuda()\n",
    "test_data = test_data.cuda()\n",
    "\n",
    "train_labels = train_labels.cuda()\n",
    "test_labels = test_labels.cuda()\n",
    "\n",
    "# Let's do a sanity check\n",
    "print(train_data.type())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you notice the slight execution delay in this operation? Yes, that's the time it took to transfer the data into GPUs memory. Also, notice that the tensor type now is `torch.cuda.FloatTensor`. To further verify that the data is actually physically existing in the GPUs memory, go to your terminal and run `$ nvidia-smi`. You should be able to see a python process listed using approx. 2GB of GPU memory. We're inching closer towards training our network.\n",
    "\n",
    "__Note__: Converting the entire data into cuda variable is NOT a good practice.\n",
    "We're still able to do it here because our data is small and can fit in\n",
    "the GPU memory. When working with larger datasets (will see tomorrow) and,\n",
    "bigger networks, it is strongly advised to convert only the minibatches into cuda just\n",
    "before they're fed to the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing `torch.autograd.Variable`\n",
    "So far, we've been dealing with PyTorch tensors very plainly. However, for them to be usable for deep learning operations, we also need to keep track of things like gradients of a tensor, if they are needed, for automatic gradient propagation. For this, we convert our tensors into objects of `torch.autograd.Variable` class. As we'll see, doing this brings our tensors to 'life', ready to handle the excruciatingly painful optimizations and backpropagation!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  2,  ...,  3,  4,  5], device='cuda:0')\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Convert a tensor to a Variable object by simply asking it to track the gradients\n",
    "train_data.requires_grad_(True)\n",
    "test_data.requires_grad_(True)\n",
    "\n",
    "# The targets/labels do not require gradients\n",
    "train_labels.requires_grad_(False)\n",
    "test_labels.requires_grad_(False)\n",
    "print(train_labels)\n",
    "print(train_labels.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Network and setting up the Loss Function\n",
    "We have created a sample network architecture for you in the file model.py. Check out its `create_fcn()` function. For initializing the losses, one may choose from a large variety of [losses available](https://pytorch.org/docs/stable/nn.html#loss-functions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Declaring some network hyperparameters\n",
    "D_in = 108*108\n",
    "D_out = 6\n",
    "\n",
    "# create_fcn function is written in model.py.\n",
    "model = create_fcn(D_in, D_out)\n",
    "# Initialise a loss function.\n",
    "# eg. if we wanted an MSE Loss: loss_fn = nn.MSELoss()\n",
    "# Please search the PyTorch doc for cross-entropy loss function\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, convert the model and the loss funtion into cuda types too. This is similar to what we did with the tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossEntropyLoss()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cuda()\n",
    "loss_fn.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing Optimizer\n",
    "An optimizer is the basic engine that performs gradient descent with all its variants and hyperparameters. We need this module to be care-free about weight updates backpropagation. PyTorch provides a wide range of optimizers buil-in. Check out [this link](https://pytorch.org/docs/stable/optim.html) to explore them. Assuming we're using Adam optimizer, we can initialize it by calling `torch.optim.Adam()`. Note that while initializing, it needs to know all the network's parameters (weights and biases). This can be provided by using `model.parameters()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "# Initializing the optimizer with hyperparameters.\n",
    "# Please play with SGD, RMSProp, Adagrad, etc.\n",
    "# Note that different optimizers may require differen hyperparameter values\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initializing some training parameters\n",
    "batch_size = 324\n",
    "\n",
    "# number of batches in one epoch\n",
    "n_batch = train_data.shape[0] // batch_size \n",
    "accuracy = 0.0\n",
    "n_epoch = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entering the Training Loop\n",
    "We'll now enter the training loop and do the following:\n",
    "* Create a minibatch of size `batch_size` from the train data\n",
    "* Forward propagate the minibatch through the network\n",
    "* Compute the loss using the lost function defined previously\n",
    "* Backpropagate the loss through the network (thanks to `torch.autograd`)\n",
    "* Update the weights of the model using the `optimizer`\n",
    "* Finally, compute the performance statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 18.262638092041016 0.0\n",
      "0 1 155.90501403808594 0.0\n",
      "0 2 196.83798217773438 0.0\n",
      "0 3 152.5719451904297 0.0\n",
      "0 4 114.1688232421875 0.0\n",
      "0 5 78.82913208007812 0.0\n",
      "0 6 82.3402099609375 0.0\n",
      "0 7 62.92122268676758 0.0\n",
      "0 8 48.19945526123047 0.0\n",
      "0 9 30.81851577758789 0.0\n",
      "0 10 24.476716995239258 0.0\n",
      "0 11 27.280046463012695 0.0\n",
      "0 12 27.58415985107422 0.0\n",
      "0 13 22.627365112304688 0.0\n",
      "0 14 22.379323959350586 0.0\n",
      "0 15 17.835065841674805 0.0\n",
      "0 16 14.883783340454102 0.0\n",
      "0 17 10.166007041931152 0.0\n",
      "0 18 7.2017927169799805 0.0\n",
      "0 19 7.916727542877197 0.0\n",
      "0 20 10.896110534667969 0.0\n",
      "0 21 7.887431621551514 0.0\n",
      "0 22 10.274028778076172 0.0\n",
      "0 23 8.752635955810547 0.0\n",
      "0 24 7.139326095581055 0.0\n",
      "0 25 7.727548122406006 0.0\n",
      "0 26 7.699992656707764 0.0\n",
      "0 27 7.140280723571777 0.0\n",
      "0 28 5.672860145568848 0.0\n",
      "0 29 4.5704264640808105 0.0\n",
      "0 30 4.390462875366211 0.0\n",
      "0 31 5.461024284362793 0.0\n",
      "0 32 5.375275611877441 0.0\n",
      "0 33 4.947575092315674 0.0\n",
      "0 34 4.124166011810303 0.0\n",
      "0 35 3.580554723739624 0.0\n",
      "0 36 3.8129467964172363 0.0\n",
      "0 37 3.9505529403686523 0.0\n",
      "0 38 4.0163092613220215 0.0\n",
      "0 39 3.84165096282959 0.0\n",
      "0 40 3.7190046310424805 0.0\n",
      "0 41 3.406144857406616 0.0\n",
      "0 42 2.9830291271209717 0.0\n",
      "0 43 3.6573030948638916 0.0\n",
      "0 44 3.502471446990967 0.0\n",
      "0 45 3.5851876735687256 0.0\n",
      "0 46 3.1489851474761963 0.0\n",
      "0 47 3.0603225231170654 0.0\n",
      "0 48 3.2157015800476074 0.0\n",
      "0 49 2.978001356124878 0.0\n",
      "0 50 3.0764060020446777 0.0\n",
      "0 51 2.882974624633789 0.0\n",
      "0 52 3.118168354034424 0.0\n",
      "0 53 2.9374616146087646 0.0\n",
      "0 54 2.9454867839813232 0.0\n",
      "0 55 2.9349048137664795 0.0\n",
      "0 56 3.355668783187866 0.0\n",
      "0 57 3.1120259761810303 0.0\n",
      "0 58 2.659132957458496 0.0\n",
      "0 59 2.984441041946411 0.0\n",
      "0 60 3.265805959701538 0.0\n",
      "0 61 2.920999050140381 0.0\n",
      "0 62 2.790292739868164 0.0\n",
      "0 63 3.18457293510437 0.0\n",
      "0 64 3.0009517669677734 0.0\n",
      "0 65 2.4778389930725098 0.0\n",
      "0 66 2.8384628295898438 0.0\n",
      "0 67 3.072361946105957 0.0\n",
      "0 68 2.5431954860687256 0.0\n",
      "0 69 2.9511518478393555 0.0\n",
      "0 70 2.911872625350952 0.0\n",
      "0 71 2.659083127975464 0.0\n",
      "0 72 2.713087320327759 0.0\n",
      "0 73 3.0116090774536133 0.0\n",
      "0 74 2.67623233795166 0.0\n",
      "0 75 2.8609535694122314 0.0\n",
      "0 76 2.702240228652954 0.0\n",
      "0 77 2.9238858222961426 0.0\n",
      "0 78 2.42897891998291 0.0\n",
      "0 79 2.9393229484558105 0.0\n",
      "0 80 2.359729051589966 0.0\n",
      "0 81 2.8249094486236572 0.0\n",
      "0 82 2.6956164836883545 0.0\n",
      "0 83 2.5873730182647705 0.0\n",
      "0 84 2.4634196758270264 0.0\n",
      "0 85 2.6655097007751465 0.0\n",
      "0 86 2.6926815509796143 0.0\n",
      "0 87 2.5702505111694336 0.0\n",
      "0 88 2.5186855792999268 0.0\n",
      "0 89 2.613605260848999 0.0\n",
      "\n",
      "*****************************************\n",
      "\n",
      "0.235\n",
      "\n",
      "*****************************************\n",
      "\n",
      "1 0 2.241562843322754 0.235\n",
      "1 1 2.6897454261779785 0.235\n",
      "1 2 2.4716708660125732 0.235\n",
      "1 3 2.387373685836792 0.235\n",
      "1 4 2.489253044128418 0.235\n",
      "1 5 2.5177526473999023 0.235\n",
      "1 6 2.458350896835327 0.235\n",
      "1 7 2.4078145027160645 0.235\n",
      "1 8 2.6252875328063965 0.235\n",
      "1 9 2.2542972564697266 0.235\n",
      "1 10 2.56972599029541 0.235\n",
      "1 11 2.392749309539795 0.235\n",
      "1 12 2.4808013439178467 0.235\n",
      "1 13 2.4425032138824463 0.235\n",
      "1 14 2.259770154953003 0.235\n",
      "1 15 2.3386504650115967 0.235\n",
      "1 16 2.381795883178711 0.235\n",
      "1 17 2.429993152618408 0.235\n",
      "1 18 2.530431032180786 0.235\n",
      "1 19 2.431203842163086 0.235\n",
      "1 20 2.3577382564544678 0.235\n",
      "1 21 2.044010639190674 0.235\n",
      "1 22 2.6106033325195312 0.235\n",
      "1 23 2.4151978492736816 0.235\n",
      "1 24 2.4629054069519043 0.235\n",
      "1 25 2.313176393508911 0.235\n",
      "1 26 2.360569953918457 0.235\n",
      "1 27 2.4414470195770264 0.235\n",
      "1 28 2.4594335556030273 0.235\n",
      "1 29 2.4526236057281494 0.235\n",
      "1 30 2.5963525772094727 0.235\n",
      "1 31 2.484360456466675 0.235\n",
      "1 32 2.4581687450408936 0.235\n",
      "1 33 2.352174758911133 0.235\n",
      "1 34 2.2839603424072266 0.235\n",
      "1 35 2.5384464263916016 0.235\n",
      "1 36 2.298344612121582 0.235\n",
      "1 37 2.194779634475708 0.235\n",
      "1 38 2.3030645847320557 0.235\n",
      "1 39 2.3811709880828857 0.235\n",
      "1 40 2.453195333480835 0.235\n",
      "1 41 2.3426077365875244 0.235\n",
      "1 42 2.342150926589966 0.235\n",
      "1 43 2.312230348587036 0.235\n",
      "1 44 2.442307472229004 0.235\n",
      "1 45 2.31685733795166 0.235\n",
      "1 46 2.3780343532562256 0.235\n",
      "1 47 2.1468749046325684 0.235\n",
      "1 48 2.179504871368408 0.235\n",
      "1 49 2.1952991485595703 0.235\n",
      "1 50 2.1766908168792725 0.235\n",
      "1 51 2.2293336391448975 0.235\n",
      "1 52 2.1627542972564697 0.235\n",
      "1 53 2.304983377456665 0.235\n",
      "1 54 2.2663445472717285 0.235\n",
      "1 55 2.3182153701782227 0.235\n",
      "1 56 2.4952478408813477 0.235\n",
      "1 57 2.357666492462158 0.235\n",
      "1 58 2.169769287109375 0.235\n",
      "1 59 2.208948850631714 0.235\n",
      "1 60 2.226287603378296 0.235\n",
      "1 61 2.280816078186035 0.235\n",
      "1 62 2.243250608444214 0.235\n",
      "1 63 2.3638370037078857 0.235\n",
      "1 64 2.3783977031707764 0.235\n",
      "1 65 2.100898504257202 0.235\n",
      "1 66 2.196361780166626 0.235\n",
      "1 67 2.239363193511963 0.235\n",
      "1 68 2.083676338195801 0.235\n",
      "1 69 2.2631354331970215 0.235\n",
      "1 70 2.339935541152954 0.235\n",
      "1 71 2.1751372814178467 0.235\n",
      "1 72 2.2702739238739014 0.235\n",
      "1 73 2.1227123737335205 0.235\n",
      "1 74 2.4098477363586426 0.235\n",
      "1 75 2.209108591079712 0.235\n",
      "1 76 2.228925943374634 0.235\n",
      "1 77 2.4313724040985107 0.235\n",
      "1 78 2.071362257003784 0.235\n",
      "1 79 2.3781373500823975 0.235\n",
      "1 80 1.9887704849243164 0.235\n",
      "1 81 2.456637382507324 0.235\n",
      "1 82 2.1811459064483643 0.235\n",
      "1 83 2.087740659713745 0.235\n",
      "1 84 2.172079086303711 0.235\n",
      "1 85 2.093871831893921 0.235\n",
      "1 86 2.2197225093841553 0.235\n",
      "1 87 2.1693124771118164 0.235\n",
      "1 88 2.019678831100464 0.235\n",
      "1 89 2.15482234954834 0.235\n",
      "2 0 1.960985541343689 0.235\n",
      "2 1 2.216156005859375 0.235\n",
      "2 2 1.9712151288986206 0.235\n",
      "2 3 2.054194688796997 0.235\n",
      "2 4 2.2028286457061768 0.235\n",
      "2 5 2.053839683532715 0.235\n",
      "2 6 2.1805553436279297 0.235\n",
      "2 7 2.095363140106201 0.235\n",
      "2 8 2.2018048763275146 0.235\n",
      "2 9 2.1086161136627197 0.235\n",
      "2 10 2.186375856399536 0.235\n",
      "2 11 2.1640446186065674 0.235\n",
      "2 12 2.1606285572052 0.235\n",
      "2 13 2.118985652923584 0.235\n",
      "2 14 1.9643139839172363 0.235\n",
      "2 15 2.0671305656433105 0.235\n",
      "2 16 2.1924214363098145 0.235\n",
      "2 17 2.125807046890259 0.235\n",
      "2 18 2.2426297664642334 0.235\n",
      "2 19 2.0435733795166016 0.235\n",
      "2 20 2.120872974395752 0.235\n",
      "2 21 1.8233461380004883 0.235\n",
      "2 22 2.2502670288085938 0.235\n",
      "2 23 2.3529975414276123 0.235\n",
      "2 24 2.2182390689849854 0.235\n",
      "2 25 2.017603635787964 0.235\n",
      "2 26 2.105745553970337 0.235\n",
      "2 27 2.1515345573425293 0.235\n",
      "2 28 2.153001308441162 0.235\n",
      "2 29 2.2235355377197266 0.235\n",
      "2 30 2.2351491451263428 0.235\n",
      "2 31 2.214203357696533 0.235\n",
      "2 32 2.145704746246338 0.235\n",
      "2 33 2.1021676063537598 0.235\n",
      "2 34 2.0587515830993652 0.235\n",
      "2 35 2.165382146835327 0.235\n",
      "2 36 2.266592025756836 0.235\n",
      "2 37 2.1442174911499023 0.235\n",
      "2 38 2.1906518936157227 0.235\n",
      "2 39 2.0777199268341064 0.235\n",
      "2 40 2.2136285305023193 0.235\n",
      "2 41 2.091001272201538 0.235\n",
      "2 42 2.0868823528289795 0.235\n",
      "2 43 2.060211181640625 0.235\n",
      "2 44 2.1372487545013428 0.235\n",
      "2 45 2.1191697120666504 0.235\n",
      "2 46 1.9949666261672974 0.235\n",
      "2 47 2.004708766937256 0.235\n",
      "2 48 2.013396739959717 0.235\n",
      "2 49 2.031620979309082 0.235\n",
      "2 50 2.096902370452881 0.235\n",
      "2 51 2.045090436935425 0.235\n",
      "2 52 2.0506770610809326 0.235\n",
      "2 53 2.148859977722168 0.235\n",
      "2 54 2.151231050491333 0.235\n",
      "2 55 2.1074671745300293 0.235\n",
      "2 56 2.431706666946411 0.235\n",
      "2 57 2.4429264068603516 0.235\n",
      "2 58 1.9799233675003052 0.235\n",
      "2 59 2.1158242225646973 0.235\n",
      "2 60 2.1760830879211426 0.235\n",
      "2 61 2.0153391361236572 0.235\n",
      "2 62 2.2031197547912598 0.235\n",
      "2 63 2.3921544551849365 0.235\n",
      "2 64 2.2001683712005615 0.235\n",
      "2 65 1.914269208908081 0.235\n",
      "2 66 2.020904302597046 0.235\n",
      "2 67 2.1453399658203125 0.235\n",
      "2 68 1.93246591091156 0.235\n",
      "2 69 2.110903263092041 0.235\n",
      "2 70 2.2986209392547607 0.235\n",
      "2 71 2.101356267929077 0.235\n",
      "2 72 2.0503456592559814 0.235\n",
      "2 73 2.0090982913970947 0.235\n",
      "2 74 2.1205172538757324 0.235\n",
      "2 75 2.043879985809326 0.235\n",
      "2 76 1.9738234281539917 0.235\n",
      "2 77 2.1233408451080322 0.235\n",
      "2 78 2.0323612689971924 0.235\n",
      "2 79 2.080914258956909 0.235\n",
      "2 80 1.8521368503570557 0.235\n",
      "2 81 2.2334532737731934 0.235\n",
      "2 82 2.062335252761841 0.235\n",
      "2 83 2.0449299812316895 0.235\n",
      "2 84 1.979779601097107 0.235\n",
      "2 85 2.0025570392608643 0.235\n",
      "2 86 2.0342459678649902 0.235\n",
      "2 87 2.163841962814331 0.235\n",
      "2 88 1.9443033933639526 0.235\n",
      "2 89 1.990356206893921 0.235\n",
      "\n",
      "*****************************************\n",
      "\n",
      "0.257\n",
      "\n",
      "*****************************************\n",
      "\n",
      "3 0 1.9729033708572388 0.257\n",
      "3 1 2.0362377166748047 0.257\n",
      "3 2 1.8815109729766846 0.257\n",
      "3 3 1.977574348449707 0.257\n",
      "3 4 2.0831925868988037 0.257\n",
      "3 5 2.008978843688965 0.257\n",
      "3 6 2.060887575149536 0.257\n",
      "3 7 2.0400099754333496 0.257\n",
      "3 8 2.0016143321990967 0.257\n",
      "3 9 1.989715576171875 0.257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 10 1.9939695596694946 0.257\n",
      "3 11 2.0041122436523438 0.257\n",
      "3 12 2.020509719848633 0.257\n",
      "3 13 2.0290138721466064 0.257\n",
      "3 14 1.9785935878753662 0.257\n",
      "3 15 1.989641785621643 0.257\n",
      "3 16 2.133155584335327 0.257\n",
      "3 17 1.9716650247573853 0.257\n",
      "3 18 2.0593690872192383 0.257\n",
      "3 19 2.0269999504089355 0.257\n",
      "3 20 1.9240705966949463 0.257\n",
      "3 21 1.7536725997924805 0.257\n",
      "3 22 2.2867319583892822 0.257\n",
      "3 23 2.150430917739868 0.257\n",
      "3 24 2.040879011154175 0.257\n",
      "3 25 1.9149091243743896 0.257\n",
      "3 26 1.9041975736618042 0.257\n",
      "3 27 1.9951214790344238 0.257\n",
      "3 28 1.9908701181411743 0.257\n",
      "3 29 2.165639638900757 0.257\n",
      "3 30 2.074413776397705 0.257\n",
      "3 31 2.073884963989258 0.257\n",
      "3 32 1.9952707290649414 0.257\n",
      "3 33 2.0119850635528564 0.257\n",
      "3 34 1.9042611122131348 0.257\n",
      "3 35 2.081861972808838 0.257\n",
      "3 36 2.3063032627105713 0.257\n",
      "3 37 1.9877722263336182 0.257\n",
      "3 38 2.0353612899780273 0.257\n",
      "3 39 1.9655901193618774 0.257\n",
      "3 40 2.0798308849334717 0.257\n",
      "3 41 2.091127634048462 0.257\n",
      "3 42 1.9187829494476318 0.257\n",
      "3 43 1.9496454000473022 0.257\n",
      "3 44 2.0331473350524902 0.257\n",
      "3 45 2.077277421951294 0.257\n",
      "3 46 1.822407841682434 0.257\n",
      "3 47 2.017019033432007 0.257\n",
      "3 48 1.9058454036712646 0.257\n",
      "3 49 1.9874202013015747 0.257\n",
      "3 50 2.0214900970458984 0.257\n",
      "3 51 1.8702151775360107 0.257\n",
      "3 52 2.0557103157043457 0.257\n",
      "3 53 2.030186414718628 0.257\n",
      "3 54 1.9399265050888062 0.257\n",
      "3 55 2.023374080657959 0.257\n",
      "3 56 2.223907470703125 0.257\n",
      "3 57 2.011658191680908 0.257\n",
      "3 58 2.009979724884033 0.257\n",
      "3 59 1.9186618328094482 0.257\n",
      "3 60 1.9301109313964844 0.257\n",
      "3 61 1.9776991605758667 0.257\n",
      "3 62 1.9784845113754272 0.257\n",
      "3 63 2.109861135482788 0.257\n",
      "3 64 2.008573293685913 0.257\n",
      "3 65 2.0508787631988525 0.257\n",
      "3 66 1.8989832401275635 0.257\n",
      "3 67 2.055881977081299 0.257\n",
      "3 68 1.8810145854949951 0.257\n",
      "3 69 1.9806424379348755 0.257\n",
      "3 70 1.9933884143829346 0.257\n",
      "3 71 1.9593886137008667 0.257\n",
      "3 72 1.9287376403808594 0.257\n",
      "3 73 1.9424431324005127 0.257\n",
      "3 74 2.141845226287842 0.257\n",
      "3 75 1.9209259748458862 0.257\n",
      "3 76 1.9458284378051758 0.257\n",
      "3 77 2.1160435676574707 0.257\n",
      "3 78 1.9809590578079224 0.257\n",
      "3 79 1.9643619060516357 0.257\n",
      "3 80 1.8936020135879517 0.257\n",
      "3 81 2.197730302810669 0.257\n",
      "3 82 1.9930646419525146 0.257\n",
      "3 83 1.973071813583374 0.257\n",
      "3 84 1.9752291440963745 0.257\n",
      "3 85 1.9443020820617676 0.257\n",
      "3 86 1.9015949964523315 0.257\n",
      "3 87 2.0531234741210938 0.257\n",
      "3 88 1.7517694234848022 0.257\n",
      "3 89 1.9824072122573853 0.257\n",
      "4 0 1.9177950620651245 0.257\n",
      "4 1 1.9355318546295166 0.257\n",
      "4 2 1.8588275909423828 0.257\n",
      "4 3 1.9378771781921387 0.257\n",
      "4 4 2.018634080886841 0.257\n",
      "4 5 1.9939311742782593 0.257\n",
      "4 6 1.990850567817688 0.257\n",
      "4 7 1.9405673742294312 0.257\n",
      "4 8 1.9740312099456787 0.257\n",
      "4 9 1.9117401838302612 0.257\n",
      "4 10 2.1041324138641357 0.257\n",
      "4 11 1.932612419128418 0.257\n",
      "4 12 1.9631906747817993 0.257\n",
      "4 13 2.1462864875793457 0.257\n",
      "4 14 1.7389857769012451 0.257\n",
      "4 15 2.1115782260894775 0.257\n",
      "4 16 1.9799712896347046 0.257\n",
      "4 17 1.8998273611068726 0.257\n",
      "4 18 2.0619585514068604 0.257\n",
      "4 19 1.8798598051071167 0.257\n",
      "4 20 1.8488208055496216 0.257\n",
      "4 21 1.7305830717086792 0.257\n",
      "4 22 2.0198917388916016 0.257\n",
      "4 23 1.985497236251831 0.257\n",
      "4 24 2.0185153484344482 0.257\n",
      "4 25 1.7972700595855713 0.257\n",
      "4 26 1.894129991531372 0.257\n",
      "4 27 1.9070193767547607 0.257\n",
      "4 28 1.9572334289550781 0.257\n",
      "4 29 2.1016204357147217 0.257\n",
      "4 30 2.042595148086548 0.257\n",
      "4 31 1.9867585897445679 0.257\n",
      "4 32 1.9538679122924805 0.257\n",
      "4 33 1.9271152019500732 0.257\n",
      "4 34 1.9059405326843262 0.257\n",
      "4 35 1.9835376739501953 0.257\n",
      "4 36 2.1697535514831543 0.257\n",
      "4 37 1.9436700344085693 0.257\n",
      "4 38 1.9103165864944458 0.257\n",
      "4 39 1.9344797134399414 0.257\n",
      "4 40 1.9947357177734375 0.257\n",
      "4 41 2.001837968826294 0.257\n",
      "4 42 1.8233156204223633 0.257\n",
      "4 43 1.8560025691986084 0.257\n",
      "4 44 1.979730248451233 0.257\n",
      "4 45 1.994496464729309 0.257\n",
      "4 46 1.7232452630996704 0.257\n",
      "4 47 1.8965295553207397 0.257\n",
      "4 48 1.8732514381408691 0.257\n",
      "4 49 1.8658651113510132 0.257\n",
      "4 50 1.9445823431015015 0.257\n",
      "4 51 1.8232953548431396 0.257\n",
      "4 52 1.9406180381774902 0.257\n",
      "4 53 1.9271641969680786 0.257\n",
      "4 54 1.8295807838439941 0.257\n",
      "4 55 1.8920772075653076 0.257\n",
      "4 56 2.1795847415924072 0.257\n",
      "4 57 1.9339054822921753 0.257\n",
      "4 58 1.868420124053955 0.257\n",
      "4 59 1.852207899093628 0.257\n",
      "4 60 1.854570746421814 0.257\n",
      "4 61 1.8989722728729248 0.257\n",
      "4 62 1.9163824319839478 0.257\n",
      "4 63 1.9982377290725708 0.257\n",
      "4 64 1.9553090333938599 0.257\n",
      "4 65 1.9053616523742676 0.257\n",
      "4 66 1.8091613054275513 0.257\n",
      "4 67 1.8906718492507935 0.257\n",
      "4 68 1.7754356861114502 0.257\n",
      "4 69 1.885723352432251 0.257\n",
      "4 70 1.894970178604126 0.257\n",
      "4 71 1.9030485153198242 0.257\n",
      "4 72 1.8426461219787598 0.257\n",
      "4 73 1.8453768491744995 0.257\n",
      "4 74 2.152681589126587 0.257\n",
      "4 75 1.9079735279083252 0.257\n",
      "4 76 1.9095131158828735 0.257\n",
      "4 77 2.107717275619507 0.257\n",
      "4 78 1.9039227962493896 0.257\n",
      "4 79 1.9896743297576904 0.257\n",
      "4 80 1.8552308082580566 0.257\n",
      "4 81 2.109767198562622 0.257\n",
      "4 82 2.0036375522613525 0.257\n",
      "4 83 1.9071775674819946 0.257\n",
      "4 84 1.9512995481491089 0.257\n",
      "4 85 1.8923407793045044 0.257\n",
      "4 86 1.8604662418365479 0.257\n",
      "4 87 1.9772166013717651 0.257\n",
      "4 88 1.7411412000656128 0.257\n",
      "4 89 1.9785431623458862 0.257\n",
      "\n",
      "*****************************************\n",
      "\n",
      "0.276\n",
      "\n",
      "*****************************************\n",
      "\n",
      "5 0 1.8037865161895752 0.276\n",
      "5 1 1.8961848020553589 0.276\n",
      "5 2 1.8173331022262573 0.276\n",
      "5 3 1.8994959592819214 0.276\n",
      "5 4 1.8955695629119873 0.276\n",
      "5 5 1.9245582818984985 0.276\n",
      "5 6 1.8842241764068604 0.276\n",
      "5 7 1.8727706670761108 0.276\n",
      "5 8 1.8454726934432983 0.276\n",
      "5 9 1.8701808452606201 0.276\n",
      "5 10 1.9783416986465454 0.276\n",
      "5 11 1.896630048751831 0.276\n",
      "5 12 1.936676263809204 0.276\n",
      "5 13 2.0537195205688477 0.276\n",
      "5 14 1.744493842124939 0.276\n",
      "5 15 2.00185489654541 0.276\n",
      "5 16 2.0230228900909424 0.276\n",
      "5 17 1.8309518098831177 0.276\n",
      "5 18 2.016559362411499 0.276\n",
      "5 19 1.8777979612350464 0.276\n",
      "5 20 1.789601445198059 0.276\n",
      "5 21 1.677629828453064 0.276\n",
      "5 22 1.8490581512451172 0.276\n",
      "5 23 1.8298135995864868 0.276\n",
      "5 24 1.9346190690994263 0.276\n",
      "5 25 1.770311713218689 0.276\n",
      "5 26 1.813963770866394 0.276\n",
      "5 27 1.8908944129943848 0.276\n",
      "5 28 1.8159325122833252 0.276\n",
      "5 29 2.0540995597839355 0.276\n",
      "5 30 1.9814225435256958 0.276\n",
      "5 31 1.948630452156067 0.276\n",
      "5 32 1.836695909500122 0.276\n",
      "5 33 1.8788702487945557 0.276\n",
      "5 34 1.8023356199264526 0.276\n",
      "5 35 1.8836826086044312 0.276\n",
      "5 36 2.0108745098114014 0.276\n",
      "5 37 1.8271254301071167 0.276\n",
      "5 38 1.8338342905044556 0.276\n",
      "5 39 1.8324865102767944 0.276\n",
      "5 40 1.9032011032104492 0.276\n",
      "5 41 1.9026658535003662 0.276\n",
      "5 42 1.766899824142456 0.276\n",
      "5 43 1.7857295274734497 0.276\n",
      "5 44 1.8978960514068604 0.276\n",
      "5 45 1.9041965007781982 0.276\n",
      "5 46 1.6752828359603882 0.276\n",
      "5 47 1.8242077827453613 0.276\n",
      "5 48 1.825647234916687 0.276\n",
      "5 49 1.808829426765442 0.276\n",
      "5 50 1.9109723567962646 0.276\n",
      "5 51 1.788318395614624 0.276\n",
      "5 52 1.8514397144317627 0.276\n",
      "5 53 1.8360300064086914 0.276\n",
      "5 54 1.7614538669586182 0.276\n",
      "5 55 1.79697847366333 0.276\n",
      "5 56 2.128593683242798 0.276\n",
      "5 57 1.9116911888122559 0.276\n",
      "5 58 1.761589527130127 0.276\n",
      "5 59 1.7694082260131836 0.276\n",
      "5 60 1.8563735485076904 0.276\n",
      "5 61 1.7693102359771729 0.276\n",
      "5 62 1.8674825429916382 0.276\n",
      "5 63 1.9405734539031982 0.276\n",
      "5 64 1.914720892906189 0.276\n",
      "5 65 1.7598376274108887 0.276\n",
      "5 66 1.7247653007507324 0.276\n",
      "5 67 1.8038963079452515 0.276\n",
      "5 68 1.691149115562439 0.276\n",
      "5 69 1.8302878141403198 0.276\n",
      "5 70 1.834599256515503 0.276\n",
      "5 71 1.7924740314483643 0.276\n",
      "5 72 1.7748183012008667 0.276\n",
      "5 73 1.8013299703598022 0.276\n",
      "5 74 2.0601186752319336 0.276\n",
      "5 75 1.8125909566879272 0.276\n",
      "5 76 1.89725923538208 0.276\n",
      "5 77 1.9963502883911133 0.276\n",
      "5 78 1.8732143640518188 0.276\n",
      "5 79 1.9492872953414917 0.276\n",
      "5 80 1.7983050346374512 0.276\n",
      "5 81 2.0368692874908447 0.276\n",
      "5 82 1.9205598831176758 0.276\n",
      "5 83 1.8951526880264282 0.276\n",
      "5 84 1.85902738571167 0.276\n",
      "5 85 1.8029288053512573 0.276\n",
      "5 86 1.8357083797454834 0.276\n",
      "5 87 1.8678172826766968 0.276\n",
      "5 88 1.688195824623108 0.276\n",
      "5 89 1.8936388492584229 0.276\n",
      "6 0 1.693249225616455 0.276\n",
      "6 1 1.8440719842910767 0.276\n",
      "6 2 1.7299954891204834 0.276\n",
      "6 3 1.7950689792633057 0.276\n",
      "6 4 1.78969144821167 0.276\n",
      "6 5 1.7922346591949463 0.276\n",
      "6 6 1.8032126426696777 0.276\n",
      "6 7 1.7625457048416138 0.276\n",
      "6 8 1.7960736751556396 0.276\n",
      "6 9 1.7561635971069336 0.276\n",
      "6 10 1.9078009128570557 0.276\n",
      "6 11 1.7403162717819214 0.276\n",
      "6 12 1.8660417795181274 0.276\n",
      "6 13 1.8938229084014893 0.276\n",
      "6 14 1.6480920314788818 0.276\n",
      "6 15 1.876178503036499 0.276\n",
      "6 16 1.792803168296814 0.276\n",
      "6 17 1.7903242111206055 0.276\n",
      "6 18 1.7835307121276855 0.276\n",
      "6 19 1.7742611169815063 0.276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 20 1.7432078123092651 0.276\n",
      "6 21 1.5640472173690796 0.276\n",
      "6 22 1.8642103672027588 0.276\n",
      "6 23 1.786619782447815 0.276\n",
      "6 24 1.7553298473358154 0.276\n",
      "6 25 1.7380467653274536 0.276\n",
      "6 26 1.6830824613571167 0.276\n",
      "6 27 1.731202483177185 0.276\n",
      "6 28 1.7254077196121216 0.276\n",
      "6 29 1.7743494510650635 0.276\n",
      "6 30 1.7170919179916382 0.276\n",
      "6 31 1.7563060522079468 0.276\n",
      "6 32 1.6645082235336304 0.276\n",
      "6 33 1.7497055530548096 0.276\n",
      "6 34 1.7057596445083618 0.276\n",
      "6 35 1.7760869264602661 0.276\n",
      "6 36 1.737547755241394 0.276\n",
      "6 37 1.7040284872055054 0.276\n",
      "6 38 1.6998168230056763 0.276\n",
      "6 39 1.7020879983901978 0.276\n",
      "6 40 1.7167904376983643 0.276\n",
      "6 41 1.7241568565368652 0.276\n",
      "6 42 1.7569421529769897 0.276\n",
      "6 43 1.6904391050338745 0.276\n",
      "6 44 1.6898781061172485 0.276\n",
      "6 45 1.7231801748275757 0.276\n",
      "6 46 1.659539818763733 0.276\n",
      "6 47 1.7212063074111938 0.276\n",
      "6 48 1.7010724544525146 0.276\n",
      "6 49 1.7331684827804565 0.276\n",
      "6 50 1.6820929050445557 0.276\n",
      "6 51 1.665595293045044 0.276\n",
      "6 52 1.682934045791626 0.276\n",
      "6 53 1.6414015293121338 0.276\n",
      "6 54 1.7266180515289307 0.276\n",
      "6 55 1.700749397277832 0.276\n",
      "6 56 1.6851868629455566 0.276\n",
      "6 57 1.718094825744629 0.276\n",
      "6 58 1.6789209842681885 0.276\n",
      "6 59 1.6576087474822998 0.276\n",
      "6 60 1.7120884656906128 0.276\n",
      "6 61 1.6986935138702393 0.276\n",
      "6 62 1.6881953477859497 0.276\n",
      "6 63 1.7000960111618042 0.276\n",
      "6 64 1.668487787246704 0.276\n",
      "6 65 1.6392743587493896 0.276\n",
      "6 66 1.6293929815292358 0.276\n",
      "6 67 1.6893573999404907 0.276\n",
      "6 68 1.6612260341644287 0.276\n",
      "6 69 1.6702520847320557 0.276\n",
      "6 70 1.6676207780838013 0.276\n",
      "6 71 1.704900860786438 0.276\n",
      "6 72 1.6626557111740112 0.276\n",
      "6 73 1.6256824731826782 0.276\n",
      "6 74 1.7049813270568848 0.276\n",
      "6 75 1.65024995803833 0.276\n",
      "6 76 1.6852232217788696 0.276\n",
      "6 77 1.6448860168457031 0.276\n",
      "6 78 1.6712514162063599 0.276\n",
      "6 79 1.650378704071045 0.276\n",
      "6 80 1.6193643808364868 0.276\n",
      "6 81 1.716295838356018 0.276\n",
      "6 82 1.6652942895889282 0.276\n",
      "6 83 1.6546269655227661 0.276\n",
      "6 84 1.6525373458862305 0.276\n",
      "6 85 1.6539632081985474 0.276\n",
      "6 86 1.66015625 0.276\n",
      "6 87 1.6299140453338623 0.276\n",
      "6 88 1.5460039377212524 0.276\n",
      "6 89 1.6242302656173706 0.276\n",
      "\n",
      "*****************************************\n",
      "\n",
      "0.284\n",
      "\n",
      "*****************************************\n",
      "\n",
      "7 0 1.6420667171478271 0.284\n",
      "7 1 1.7212135791778564 0.284\n",
      "7 2 1.6630710363388062 0.284\n",
      "7 3 1.662982702255249 0.284\n",
      "7 4 1.6824992895126343 0.284\n",
      "7 5 1.6284757852554321 0.284\n",
      "7 6 1.6729878187179565 0.284\n",
      "7 7 1.6257843971252441 0.284\n",
      "7 8 1.7206875085830688 0.284\n",
      "7 9 1.6254454851150513 0.284\n",
      "7 10 1.6733427047729492 0.284\n",
      "7 11 1.6481901407241821 0.284\n",
      "7 12 1.6336969137191772 0.284\n",
      "7 13 1.6126179695129395 0.284\n",
      "7 14 1.625876545906067 0.284\n",
      "7 15 1.6290779113769531 0.284\n",
      "7 16 1.6342334747314453 0.284\n",
      "7 17 1.6093950271606445 0.284\n",
      "7 18 1.6504793167114258 0.284\n",
      "7 19 1.6256502866744995 0.284\n",
      "7 20 1.6180686950683594 0.284\n",
      "7 21 1.5520979166030884 0.284\n",
      "7 22 1.6664924621582031 0.284\n",
      "7 23 1.6602777242660522 0.284\n",
      "7 24 1.6252094507217407 0.284\n",
      "7 25 1.60538911819458 0.284\n",
      "7 26 1.6154675483703613 0.284\n",
      "7 27 1.6287033557891846 0.284\n",
      "7 28 1.6548393964767456 0.284\n",
      "7 29 1.6956892013549805 0.284\n",
      "7 30 1.6546725034713745 0.284\n",
      "7 31 1.6885594129562378 0.284\n",
      "7 32 1.6309505701065063 0.284\n",
      "7 33 1.7011682987213135 0.284\n",
      "7 34 1.68890380859375 0.284\n",
      "7 35 1.7035177946090698 0.284\n",
      "7 36 1.6505824327468872 0.284\n",
      "7 37 1.6709256172180176 0.284\n",
      "7 38 1.666569471359253 0.284\n",
      "7 39 1.6525436639785767 0.284\n",
      "7 40 1.6580878496170044 0.284\n",
      "7 41 1.6735174655914307 0.284\n",
      "7 42 1.6872601509094238 0.284\n",
      "7 43 1.6421654224395752 0.284\n",
      "7 44 1.6490906476974487 0.284\n",
      "7 45 1.6233482360839844 0.284\n",
      "7 46 1.6007134914398193 0.284\n",
      "7 47 1.6380560398101807 0.284\n",
      "7 48 1.6151490211486816 0.284\n",
      "7 49 1.6842050552368164 0.284\n",
      "7 50 1.6665574312210083 0.284\n",
      "7 51 1.5758448839187622 0.284\n",
      "7 52 1.6182094812393188 0.284\n",
      "7 53 1.6336374282836914 0.284\n",
      "7 54 1.6746070384979248 0.284\n",
      "7 55 1.5980228185653687 0.284\n",
      "7 56 1.673304557800293 0.284\n",
      "7 57 1.6576415300369263 0.284\n",
      "7 58 1.5965114831924438 0.284\n",
      "7 59 1.612439751625061 0.284\n",
      "7 60 1.687659502029419 0.284\n",
      "7 61 1.6461526155471802 0.284\n",
      "7 62 1.6352025270462036 0.284\n",
      "7 63 1.663561224937439 0.284\n",
      "7 64 1.6225005388259888 0.284\n",
      "7 65 1.5348851680755615 0.284\n",
      "7 66 1.5570533275604248 0.284\n",
      "7 67 1.6390361785888672 0.284\n",
      "7 68 1.598626732826233 0.284\n",
      "7 69 1.6472746133804321 0.284\n",
      "7 70 1.6268911361694336 0.284\n",
      "7 71 1.6794642210006714 0.284\n",
      "7 72 1.64841890335083 0.284\n",
      "7 73 1.5935747623443604 0.284\n",
      "7 74 1.677886724472046 0.284\n",
      "7 75 1.6656494140625 0.284\n",
      "7 76 1.661321997642517 0.284\n",
      "7 77 1.626832365989685 0.284\n",
      "7 78 1.665307879447937 0.284\n",
      "7 79 1.5945593118667603 0.284\n",
      "7 80 1.6022276878356934 0.284\n",
      "7 81 1.6801244020462036 0.284\n",
      "7 82 1.6338289976119995 0.284\n",
      "7 83 1.6384131908416748 0.284\n",
      "7 84 1.6158589124679565 0.284\n",
      "7 85 1.624316692352295 0.284\n",
      "7 86 1.6160956621170044 0.284\n",
      "7 87 1.5761239528656006 0.284\n",
      "7 88 1.5338324308395386 0.284\n",
      "7 89 1.5908846855163574 0.284\n",
      "8 0 1.5574239492416382 0.284\n",
      "8 1 1.6771084070205688 0.284\n",
      "8 2 1.600563883781433 0.284\n",
      "8 3 1.60690176486969 0.284\n",
      "8 4 1.600731611251831 0.284\n",
      "8 5 1.6182793378829956 0.284\n",
      "8 6 1.6404539346694946 0.284\n",
      "8 7 1.5911349058151245 0.284\n",
      "8 8 1.6445766687393188 0.284\n",
      "8 9 1.5688790082931519 0.284\n",
      "8 10 1.6244474649429321 0.284\n",
      "8 11 1.6105488538742065 0.284\n",
      "8 12 1.6012589931488037 0.284\n",
      "8 13 1.5467504262924194 0.284\n",
      "8 14 1.6073623895645142 0.284\n",
      "8 15 1.6070468425750732 0.284\n",
      "8 16 1.6057589054107666 0.284\n",
      "8 17 1.6043734550476074 0.284\n",
      "8 18 1.6327624320983887 0.284\n",
      "8 19 1.6125527620315552 0.284\n",
      "8 20 1.5747262239456177 0.284\n",
      "8 21 1.5203101634979248 0.284\n",
      "8 22 1.6310091018676758 0.284\n",
      "8 23 1.6273647546768188 0.284\n",
      "8 24 1.6303688287734985 0.284\n",
      "8 25 1.547943353652954 0.284\n",
      "8 26 1.6212738752365112 0.284\n",
      "8 27 1.5903888940811157 0.284\n",
      "8 28 1.6176316738128662 0.284\n",
      "8 29 1.6332237720489502 0.284\n",
      "8 30 1.6190402507781982 0.284\n",
      "8 31 1.675916075706482 0.284\n",
      "8 32 1.580944299697876 0.284\n",
      "8 33 1.664513349533081 0.284\n",
      "8 34 1.654517412185669 0.284\n",
      "8 35 1.6489067077636719 0.284\n",
      "8 36 1.622172236442566 0.284\n",
      "8 37 1.6368696689605713 0.284\n",
      "8 38 1.6551121473312378 0.284\n",
      "8 39 1.6339150667190552 0.284\n",
      "8 40 1.6377222537994385 0.284\n",
      "8 41 1.6507195234298706 0.284\n",
      "8 42 1.6341711282730103 0.284\n",
      "8 43 1.6376687288284302 0.284\n",
      "8 44 1.6389237642288208 0.284\n",
      "8 45 1.5885944366455078 0.284\n",
      "8 46 1.5983078479766846 0.284\n",
      "8 47 1.6362090110778809 0.284\n",
      "8 48 1.5841600894927979 0.284\n",
      "8 49 1.6890193223953247 0.284\n",
      "8 50 1.667829155921936 0.284\n",
      "8 51 1.541153073310852 0.284\n",
      "8 52 1.575218915939331 0.284\n",
      "8 53 1.575011968612671 0.284\n",
      "8 54 1.6743319034576416 0.284\n",
      "8 55 1.5494579076766968 0.284\n",
      "8 56 1.6393587589263916 0.284\n",
      "8 57 1.6014419794082642 0.284\n",
      "8 58 1.561987280845642 0.284\n",
      "8 59 1.5947597026824951 0.284\n",
      "8 60 1.6562185287475586 0.284\n",
      "8 61 1.599930763244629 0.284\n",
      "8 62 1.6379715204238892 0.284\n",
      "8 63 1.642254114151001 0.284\n",
      "8 64 1.5761029720306396 0.284\n",
      "8 65 1.4869873523712158 0.284\n",
      "8 66 1.5298669338226318 0.284\n",
      "8 67 1.5884408950805664 0.284\n",
      "8 68 1.5864131450653076 0.284\n",
      "8 69 1.6220742464065552 0.284\n",
      "8 70 1.645423173904419 0.284\n",
      "8 71 1.6087442636489868 0.284\n",
      "8 72 1.6274794340133667 0.284\n",
      "8 73 1.551379680633545 0.284\n",
      "8 74 1.652679443359375 0.284\n",
      "8 75 1.6139347553253174 0.284\n",
      "8 76 1.6195825338363647 0.284\n",
      "8 77 1.5967880487442017 0.284\n",
      "8 78 1.6066008806228638 0.284\n",
      "8 79 1.5786986351013184 0.284\n",
      "8 80 1.5206103324890137 0.284\n",
      "8 81 1.6364268064498901 0.284\n",
      "8 82 1.560639500617981 0.284\n",
      "8 83 1.5968579053878784 0.284\n",
      "8 84 1.5999115705490112 0.284\n",
      "8 85 1.6008999347686768 0.284\n",
      "8 86 1.5597853660583496 0.284\n",
      "8 87 1.5512592792510986 0.284\n",
      "8 88 1.5025603771209717 0.284\n",
      "8 89 1.559779405593872 0.284\n",
      "\n",
      "*****************************************\n",
      "\n",
      "0.307\n",
      "\n",
      "*****************************************\n",
      "\n",
      "9 0 1.5456411838531494 0.307\n",
      "9 1 1.6255990266799927 0.307\n",
      "9 2 1.5861926078796387 0.307\n",
      "9 3 1.5689878463745117 0.307\n",
      "9 4 1.5636826753616333 0.307\n",
      "9 5 1.5722190141677856 0.307\n",
      "9 6 1.5936689376831055 0.307\n",
      "9 7 1.5579692125320435 0.307\n",
      "9 8 1.6048011779785156 0.307\n",
      "9 9 1.5660955905914307 0.307\n",
      "9 10 1.5974397659301758 0.307\n",
      "9 11 1.5670338869094849 0.307\n",
      "9 12 1.5547385215759277 0.307\n",
      "9 13 1.5274584293365479 0.307\n",
      "9 14 1.5616573095321655 0.307\n",
      "9 15 1.563019037246704 0.307\n",
      "9 16 1.6022560596466064 0.307\n",
      "9 17 1.5901015996932983 0.307\n",
      "9 18 1.5932775735855103 0.307\n",
      "9 19 1.5549230575561523 0.307\n",
      "9 20 1.5438095331192017 0.307\n",
      "9 21 1.496273159980774 0.307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 22 1.6727015972137451 0.307\n",
      "9 23 1.601629614830017 0.307\n",
      "9 24 1.5963634252548218 0.307\n",
      "9 25 1.5189048051834106 0.307\n",
      "9 26 1.5652315616607666 0.307\n",
      "9 27 1.5828133821487427 0.307\n",
      "9 28 1.5669399499893188 0.307\n",
      "9 29 1.6210089921951294 0.307\n",
      "9 30 1.582269549369812 0.307\n",
      "9 31 1.6595406532287598 0.307\n",
      "9 32 1.5691709518432617 0.307\n",
      "9 33 1.6524970531463623 0.307\n",
      "9 34 1.6485130786895752 0.307\n",
      "9 35 1.6219627857208252 0.307\n",
      "9 36 1.595784306526184 0.307\n",
      "9 37 1.5948957204818726 0.307\n",
      "9 38 1.6385635137557983 0.307\n",
      "9 39 1.5939081907272339 0.307\n",
      "9 40 1.6111139059066772 0.307\n",
      "9 41 1.6085941791534424 0.307\n",
      "9 42 1.6252349615097046 0.307\n",
      "9 43 1.600885033607483 0.307\n",
      "9 44 1.6436967849731445 0.307\n",
      "9 45 1.5398772954940796 0.307\n",
      "9 46 1.5506510734558105 0.307\n",
      "9 47 1.6348074674606323 0.307\n",
      "9 48 1.5811620950698853 0.307\n",
      "9 49 1.6153044700622559 0.307\n",
      "9 50 1.6089204549789429 0.307\n",
      "9 51 1.5259758234024048 0.307\n",
      "9 52 1.5560765266418457 0.307\n",
      "9 53 1.565075397491455 0.307\n",
      "9 54 1.612939476966858 0.307\n",
      "9 55 1.513331651687622 0.307\n",
      "9 56 1.6106313467025757 0.307\n",
      "9 57 1.57547128200531 0.307\n",
      "9 58 1.5243501663208008 0.307\n",
      "9 59 1.57887864112854 0.307\n",
      "9 60 1.6520427465438843 0.307\n",
      "9 61 1.5932623147964478 0.307\n",
      "9 62 1.6036484241485596 0.307\n",
      "9 63 1.6030163764953613 0.307\n",
      "9 64 1.5445091724395752 0.307\n",
      "9 65 1.4684265851974487 0.307\n",
      "9 66 1.50127375125885 0.307\n",
      "9 67 1.5784385204315186 0.307\n",
      "9 68 1.560730218887329 0.307\n",
      "9 69 1.5902775526046753 0.307\n",
      "9 70 1.6209867000579834 0.307\n",
      "9 71 1.5831336975097656 0.307\n",
      "9 72 1.588860034942627 0.307\n",
      "9 73 1.5286344289779663 0.307\n",
      "9 74 1.6378732919692993 0.307\n",
      "9 75 1.6015456914901733 0.307\n",
      "9 76 1.6158961057662964 0.307\n",
      "9 77 1.5906859636306763 0.307\n",
      "9 78 1.6017117500305176 0.307\n",
      "9 79 1.594330906867981 0.307\n",
      "9 80 1.500264286994934 0.307\n",
      "9 81 1.628185510635376 0.307\n",
      "9 82 1.5435208082199097 0.307\n",
      "9 83 1.5790880918502808 0.307\n",
      "9 84 1.5952404737472534 0.307\n",
      "9 85 1.6058417558670044 0.307\n",
      "9 86 1.5594269037246704 0.307\n",
      "9 87 1.5476847887039185 0.307\n",
      "9 88 1.5114891529083252 0.307\n",
      "9 89 1.538836121559143 0.307\n"
     ]
    }
   ],
   "source": [
    "# Before we enter the training loop, remember that the shape of our\n",
    "# input data is (29160, 1, 108, 108). This shape is perfect if using\n",
    "# a CNN. But, for a fully-connected network, the model expects an \n",
    "# input of (minibatch_size, input_dim). Therefore, the (1, 108, 108)\n",
    "# dimensional image needs to be flattened out before feeding to the \n",
    "# network.\n",
    "\n",
    "train_data = train_data.reshape(-1, 108*108)\n",
    "test_data = test_data.reshape(-1, 108*108)\n",
    "\n",
    "for t in range(n_epoch):\n",
    "     for m in range(n_batch):\n",
    "         inp = train_data[m * batch_size: (m+1) * batch_size]\n",
    "         tar = train_labels[m * batch_size: (m+1) * batch_size ]\n",
    " \n",
    "         # Add random perturbations in this functions. Define\n",
    "         # this function if you wish to use it.\n",
    "         # inp = add_noise(inp)\n",
    " \n",
    "         # Compute the network's output: Forward Prop\n",
    "         pred = model(inp)\n",
    " \n",
    "         # Compute the network's loss\n",
    "         loss = loss_fn(pred, tar)\n",
    " \n",
    "         # Zero the gradients of all the network's parameters\n",
    "         optimizer.zero_grad()\n",
    " \n",
    "         # Computer the network's gradients: Backward Prop\n",
    "         loss.backward()\n",
    " \n",
    "         # Update the network's parameters based on the computed\n",
    "         # gradients\n",
    "         optimizer.step()\n",
    " \n",
    "         print(t, m, loss.item(), accuracy)\n",
    " \n",
    "     # Validation after every 2nd epoch\n",
    "     if t % 2 == 0:\n",
    "         # Forward pass\n",
    "         output = model(test_data)\n",
    " \n",
    "         # get the index of the max log-probability\n",
    "         pred = output.data.max(1)[1]\n",
    " \n",
    "         correct = pred.eq(test_labels).sum()\n",
    "         accuracy = correct.item() / 1000\n",
    "         print(\"\\n*****************************************\\n\")\n",
    "         print(accuracy)\n",
    "         print(\"\\n*****************************************\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's our introduction to neural networks using PyTorch. Tomorrow, we'll try solving a more challenging problem with bigger dataset and more complicated network in a more principled manner! Hope to see you all tomorrow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
