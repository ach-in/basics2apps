{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from model import Lenet as Lenet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "The data has been arranged in train and test directories. train.npy and test.npy contain the input images for the test and train sets. Likewise, train_cat.npy and test_cat.npy contain the corresponding labels.\n",
    "We first load the data using `np.load()` and check out the shapes of numpy arrays using `np.shape`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29160, 1, 108, 108) (29160,)\n",
      "(29160, 1, 108, 108) (29160,)\n",
      "<class 'numpy.ndarray'>\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "train_data = np.load('norb/train.npy')\n",
    "train_labels = np.load('norb/train_cat.npy')\n",
    "\n",
    "test_data = np.load('norb/test.npy')\n",
    "test_labels = np.load('norb/test_cat.npy')\n",
    "\n",
    "# Let's print the shapes of these numpy arrays\n",
    "print(train_data.shape, train_labels.shape)\n",
    "print(test_data.shape, test_labels.shape)\n",
    "\n",
    "\n",
    "# Let's also check the data type of these variables\n",
    "print(type(train_data))\n",
    "for i in range(10):\n",
    "    print(train_labels[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the test data is quite large in size. We'll deal with a small subset of the test set for validation and use the rest for testing later. For this, we'll have to slice the test set in its first dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1, 108, 108) (1000,)\n"
     ]
    }
   ],
   "source": [
    "# Choosing a subset (first 1000) of the test set for validation purposes.\n",
    "test_data = test_data[:1000]\n",
    "test_labels = test_labels[:1000]\n",
    "\n",
    "# Let's verify\n",
    "print(test_data.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting our data from numpy arrays to PyTorch tensors.\n",
    "So far, we've been working with numpy arrays. For performing further operations, like forward prop, accessing cuda, we should convert the numpy arrays into PyTorch tensors. It's simple: use `torch.from_numpy()` for this. Typecasting can be accomplished by simply calling the corresponding functions: `x.float()` or `x.long()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# Converting to PyTorch tensors\n",
    "train_data = torch.from_numpy(train_data).float()\n",
    "test_data = torch.from_numpy(test_data).float()\n",
    "\n",
    "# If we're planning to use cross entropy loss, the data type of the\n",
    "# targets needs to be 'long'.\n",
    "train_labels = torch.from_numpy(train_labels).long()\n",
    "test_labels = torch.from_numpy(test_labels).long()\n",
    "\n",
    "print(type(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([29160, 1, 108, 108])\n",
      "29160\n"
     ]
    }
   ],
   "source": [
    "# When dealing with PyTorch tensors, it is recommended to use x.size()\n",
    "# instead of x.shape to find the shape/size of the tensor\n",
    "print(train_data.size())\n",
    "print(train_data.size(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting into Cuda tensors\n",
    "Since we're going to use GPUs, the variables first need to be converted to cuda-type. For doing this, use `x = x.cuda()`. This, in effect, loads the tensors into your GPUs memory. This operation should be used very judiciously because if mishandled the data transfer itself could introduce major time delays. More on this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "# Convert the data and labels into cuda Variables now: x = x.cuda()\n",
    "train_data = train_data.cuda()\n",
    "test_data = test_data.cuda()\n",
    "\n",
    "train_labels = train_labels.cuda()\n",
    "test_labels = test_labels.cuda()\n",
    "\n",
    "# Let's do a sanity check\n",
    "print(train_data.type())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you notice the slight execution delay in this operation? Yes, that's the time it took to transfer the data into GPUs memory. Also, notice that the tensor type now is `torch.cuda.FloatTensor`. To further verify that the data is actually physically existing in the GPUs memory, go to your terminal and run `$ nvidia-smi`. You should be able to see a python process listed using approx. 2GB of GPU memory. We're inching closer towards training our network.\n",
    "\n",
    "__Note__: Converting the entire data into cuda variable is NOT a good practice.\n",
    "We're still able to do it here because our data is small and can fit in\n",
    "the GPU memory. When working with larger datasets (will see tomorrow) and,\n",
    "bigger networks, it is strongly advised to convert only the minibatches into cuda just\n",
    "before they're fed to the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing `torch.autograd.Variable`\n",
    "So far, we've been dealing with PyTorch tensors very plainly. However, for them to be usable for deep learning operations, we also need to keep track of things like gradients of a tensor, if they are needed, for automatic gradient propagation. For this, we convert our tensors into objects of `torch.autograd.Variable` class. As we'll see, doing this brings our tensors to 'life', ready to handle the excruciatingly painful optimizations and backpropagation!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  2,  ...,  3,  4,  5], device='cuda:0')\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Convert a tensor to a Variable object by simply asking it to track the gradients\n",
    "train_data.requires_grad_(True)\n",
    "test_data.requires_grad_(True)\n",
    "\n",
    "# The targets/labels do not require gradients\n",
    "train_labels.requires_grad_(False)\n",
    "test_labels.requires_grad_(False)\n",
    "print(train_labels)\n",
    "print(train_labels.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Network and setting up the Loss Function\n",
    "We have created a sample network architecture for you in the file model.py. Check out its `create_fcn()` function. For initializing the losses, one may choose from a large variety of [losses available](https://pytorch.org/docs/stable/nn.html#loss-functions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create_fcn function is written in model.py.\n",
    "model = Lenet()\n",
    "# Initialise a loss function.\n",
    "# eg. if we wanted an MSE Loss: loss_fn = nn.MSELoss()\n",
    "# Please search the PyTorch doc for cross-entropy loss function\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, convert the model and the loss funtion into cuda types too. This is similar to what we did with the tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossEntropyLoss()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cuda()\n",
    "loss_fn.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing Optimizer\n",
    "An optimizer is the basic engine that performs gradient descent with all its variants and hyperparameters. We need this module to be care-free about weight updates backpropagation. PyTorch provides a wide range of optimizers buil-in. Check out [this link](https://pytorch.org/docs/stable/optim.html) to explore them. Assuming we're using Adam optimizer, we can initialize it by calling `torch.optim.Adam()`. Note that while initializing, it needs to know all the network's parameters (weights and biases). This can be provided by using `model.parameters()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "# Initializing the optimizer with hyperparameters.\n",
    "# Please play with SGD, RMSProp, Adagrad, etc.\n",
    "# Note that different optimizers may require differen hyperparameter values\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initializing some training parameters\n",
    "batch_size = 324\n",
    "\n",
    "# number of batches in one epoch\n",
    "n_batch = train_data.shape[0] // batch_size \n",
    "accuracy = 0.0\n",
    "n_epoch = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entering the Training Loop\n",
    "We'll now enter the training loop and do the following:\n",
    "* Create a minibatch of size `batch_size` from the train data\n",
    "* Forward propagate the minibatch through the network\n",
    "* Compute the loss using the lost function defined previously\n",
    "* Backpropagate the loss through the network (thanks to `torch.autograd`)\n",
    "* Update the weights of the model using the `optimizer`\n",
    "* Finally, compute the performance statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([29160, 1, 108, 108])\n",
      "0 0 0.4715828001499176 0.0\n",
      "0 1 0.43570002913475037 0.0\n",
      "0 2 0.4303221106529236 0.0\n",
      "0 3 0.5127412676811218 0.0\n",
      "0 4 0.5315393805503845 0.0\n",
      "0 5 0.45340511202812195 0.0\n",
      "0 6 0.42806583642959595 0.0\n",
      "0 7 0.434641569852829 0.0\n",
      "0 8 0.48168861865997314 0.0\n",
      "0 9 0.47047993540763855 0.0\n",
      "0 10 0.4525861442089081 0.0\n",
      "0 11 0.4219578504562378 0.0\n",
      "0 12 0.4365914463996887 0.0\n",
      "0 13 0.43112102150917053 0.0\n",
      "0 14 0.3594200909137726 0.0\n",
      "0 15 0.4594959318637848 0.0\n",
      "0 16 0.4714396297931671 0.0\n",
      "0 17 0.43740707635879517 0.0\n",
      "0 18 0.42703863978385925 0.0\n",
      "0 19 0.400892972946167 0.0\n",
      "0 20 0.5120314359664917 0.0\n",
      "0 21 0.4627264142036438 0.0\n",
      "0 22 0.5162467360496521 0.0\n",
      "0 23 0.48216646909713745 0.0\n",
      "0 24 0.4687061905860901 0.0\n",
      "0 25 0.4201838970184326 0.0\n",
      "0 26 0.4723787307739258 0.0\n",
      "0 27 0.4708762466907501 0.0\n",
      "0 28 0.44528627395629883 0.0\n",
      "0 29 0.3720390796661377 0.0\n",
      "0 30 0.5000483393669128 0.0\n",
      "0 31 0.4863823652267456 0.0\n",
      "0 32 0.36215898394584656 0.0\n",
      "0 33 0.49830862879753113 0.0\n",
      "0 34 0.470184326171875 0.0\n",
      "0 35 0.4638444781303406 0.0\n",
      "0 36 0.48371341824531555 0.0\n",
      "0 37 0.4119345545768738 0.0\n",
      "0 38 0.43199416995048523 0.0\n",
      "0 39 0.4806131422519684 0.0\n",
      "0 40 0.44890546798706055 0.0\n",
      "0 41 0.43735677003860474 0.0\n",
      "0 42 0.38433682918548584 0.0\n",
      "0 43 0.468954473733902 0.0\n",
      "0 44 0.4410346746444702 0.0\n",
      "0 45 0.4130572974681854 0.0\n",
      "0 46 0.3852841556072235 0.0\n",
      "0 47 0.4916900396347046 0.0\n",
      "0 48 0.3782837390899658 0.0\n",
      "0 49 0.3886833190917969 0.0\n",
      "0 50 0.4084329903125763 0.0\n",
      "0 51 0.4318733811378479 0.0\n",
      "0 52 0.4355003833770752 0.0\n",
      "0 53 0.4149937927722931 0.0\n",
      "0 54 0.4057222902774811 0.0\n",
      "0 55 0.3833819329738617 0.0\n",
      "0 56 0.38282153010368347 0.0\n",
      "0 57 0.3609752953052521 0.0\n",
      "0 58 0.3793053925037384 0.0\n",
      "0 59 0.3246385455131531 0.0\n",
      "0 60 0.4696868062019348 0.0\n",
      "0 61 0.37479373812675476 0.0\n",
      "0 62 0.3954232633113861 0.0\n",
      "0 63 0.4250114858150482 0.0\n",
      "0 64 0.3573991656303406 0.0\n",
      "0 65 0.36596426367759705 0.0\n",
      "0 66 0.37227198481559753 0.0\n",
      "0 67 0.36828309297561646 0.0\n",
      "0 68 0.3525741696357727 0.0\n",
      "0 69 0.4376490116119385 0.0\n",
      "0 70 0.44354164600372314 0.0\n",
      "0 71 0.4156709909439087 0.0\n",
      "0 72 0.3487623333930969 0.0\n",
      "0 73 0.3757280111312866 0.0\n",
      "0 74 0.4209508001804352 0.0\n",
      "0 75 0.4124237298965454 0.0\n",
      "0 76 0.3964384198188782 0.0\n",
      "0 77 0.37780454754829407 0.0\n",
      "0 78 0.2918333113193512 0.0\n",
      "0 79 0.43500274419784546 0.0\n",
      "0 80 0.42547371983528137 0.0\n",
      "0 81 0.3712201416492462 0.0\n",
      "0 82 0.3942883014678955 0.0\n",
      "0 83 0.4457157850265503 0.0\n",
      "0 84 0.4128180742263794 0.0\n",
      "0 85 0.41816094517707825 0.0\n",
      "0 86 0.425445556640625 0.0\n",
      "0 87 0.4895307719707489 0.0\n",
      "0 88 0.44256576895713806 0.0\n",
      "0 89 0.41914406418800354 0.0\n",
      "\n",
      "*****************************************\n",
      "\n",
      "0.788\n",
      "\n",
      "*****************************************\n",
      "\n",
      "1 0 0.3919537365436554 0.788\n",
      "1 1 0.4108870327472687 0.788\n",
      "1 2 0.4200494587421417 0.788\n",
      "1 3 0.45070573687553406 0.788\n",
      "1 4 0.42057517170906067 0.788\n",
      "1 5 0.44422176480293274 0.788\n",
      "1 6 0.40612703561782837 0.788\n",
      "1 7 0.3566249907016754 0.788\n",
      "1 8 0.3951131999492645 0.788\n",
      "1 9 0.46141400933265686 0.788\n",
      "1 10 0.45466160774230957 0.788\n",
      "1 11 0.33681145310401917 0.788\n",
      "1 12 0.3589860498905182 0.788\n",
      "1 13 0.4395180642604828 0.788\n",
      "1 14 0.3742643892765045 0.788\n",
      "1 15 0.40319156646728516 0.788\n",
      "1 16 0.3840200901031494 0.788\n",
      "1 17 0.4241793155670166 0.788\n",
      "1 18 0.44488388299942017 0.788\n",
      "1 19 0.37817203998565674 0.788\n",
      "1 20 0.4159749746322632 0.788\n",
      "1 21 0.42136383056640625 0.788\n",
      "1 22 0.5153624415397644 0.788\n",
      "1 23 0.4650651514530182 0.788\n",
      "1 24 0.44372183084487915 0.788\n",
      "1 25 0.3642696738243103 0.788\n",
      "1 26 0.4423087239265442 0.788\n",
      "1 27 0.4992011785507202 0.788\n",
      "1 28 0.44800153374671936 0.788\n",
      "1 29 0.32100600004196167 0.788\n",
      "1 30 0.4494054317474365 0.788\n",
      "1 31 0.49699947237968445 0.788\n",
      "1 32 0.36789843440055847 0.788\n",
      "1 33 0.45024609565734863 0.788\n",
      "1 34 0.4167618453502655 0.788\n",
      "1 35 0.4417288601398468 0.788\n",
      "1 36 0.4872824549674988 0.788\n",
      "1 37 0.40406355261802673 0.788\n",
      "1 38 0.3978027105331421 0.788\n",
      "1 39 0.4532800018787384 0.788\n",
      "1 40 0.4328947067260742 0.788\n",
      "1 41 0.44128119945526123 0.788\n",
      "1 42 0.36486825346946716 0.788\n",
      "1 43 0.4345938265323639 0.788\n",
      "1 44 0.410384863615036 0.788\n",
      "1 45 0.3956319987773895 0.788\n",
      "1 46 0.3669479191303253 0.788\n",
      "1 47 0.47103533148765564 0.788\n",
      "1 48 0.34988197684288025 0.788\n",
      "1 49 0.36500564217567444 0.788\n",
      "1 50 0.36367112398147583 0.788\n",
      "1 51 0.4205797016620636 0.788\n",
      "1 52 0.42780792713165283 0.788\n",
      "1 53 0.3908661901950836 0.788\n",
      "1 54 0.38534626364707947 0.788\n",
      "1 55 0.38839420676231384 0.788\n",
      "1 56 0.3595581352710724 0.788\n",
      "1 57 0.3553007245063782 0.788\n",
      "1 58 0.3866884410381317 0.788\n",
      "1 59 0.3187267482280731 0.788\n",
      "1 60 0.4252485930919647 0.788\n",
      "1 61 0.3695526421070099 0.788\n",
      "1 62 0.395034521818161 0.788\n",
      "1 63 0.45870038866996765 0.788\n",
      "1 64 0.3289123475551605 0.788\n",
      "1 65 0.32133111357688904 0.788\n",
      "1 66 0.35692355036735535 0.788\n",
      "1 67 0.37899717688560486 0.788\n",
      "1 68 0.34523218870162964 0.788\n",
      "1 69 0.4204285740852356 0.788\n",
      "1 70 0.40999099612236023 0.788\n",
      "1 71 0.40211015939712524 0.788\n",
      "1 72 0.336748331785202 0.788\n",
      "1 73 0.3635055422782898 0.788\n",
      "1 74 0.37158167362213135 0.788\n",
      "1 75 0.37678417563438416 0.788\n",
      "1 76 0.3617553114891052 0.788\n",
      "1 77 0.34199586510658264 0.788\n",
      "1 78 0.2871873080730438 0.788\n",
      "1 79 0.38314706087112427 0.788\n",
      "1 80 0.3605356812477112 0.788\n",
      "1 81 0.34308719635009766 0.788\n",
      "1 82 0.39002665877342224 0.788\n",
      "1 83 0.3998376429080963 0.788\n",
      "1 84 0.36045488715171814 0.788\n",
      "1 85 0.3978806138038635 0.788\n",
      "1 86 0.39565014839172363 0.788\n",
      "1 87 0.4681704044342041 0.788\n",
      "1 88 0.3897436857223511 0.788\n",
      "1 89 0.3838764727115631 0.788\n"
     ]
    }
   ],
   "source": [
    "print(train_data.size())\n",
    "for t in range(n_epoch):\n",
    "     for m in range(n_batch):\n",
    "         inp = train_data[m * batch_size: (m+1) * batch_size]\n",
    "         tar = train_labels[m * batch_size: (m+1) * batch_size ]\n",
    " \n",
    "         # Add random perturbations in this functions. Define\n",
    "         # this function if you wish to use it.\n",
    "         # inp = add_noise(inp)\n",
    " \n",
    "         # Compute the network's output: Forward Prop\n",
    "         pred = model(inp)\n",
    " \n",
    "         # Compute the network's loss\n",
    "         loss = loss_fn(pred, tar)\n",
    " \n",
    "         # Zero the gradients of all the network's parameters\n",
    "         optimizer.zero_grad()\n",
    " \n",
    "         # Computer the network's gradients: Backward Prop\n",
    "         loss.backward()\n",
    " \n",
    "         # Update the network's parameters based on the computed\n",
    "         # gradients\n",
    "         optimizer.step()\n",
    " \n",
    "         print(t, m, loss.item(), accuracy)\n",
    " \n",
    "     # Validation after every 2nd epoch\n",
    "     if t % 2 == 0:\n",
    "         # Forward pass\n",
    "         output = model(test_data)\n",
    " \n",
    "         # get the index of the max log-probability\n",
    "         pred = output.data.max(1)[1]\n",
    " \n",
    "         correct = pred.eq(test_labels).sum()\n",
    "         accuracy = correct.item() / 1000\n",
    "         print(\"\\n*****************************************\\n\")\n",
    "         print(accuracy)\n",
    "         print(\"\\n*****************************************\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's our introduction to neural networks using PyTorch. Tomorrow, we'll try solving a more challenging problem with bigger dataset and more complicated network in a more principled manner! Hope to see you all tomorrow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
