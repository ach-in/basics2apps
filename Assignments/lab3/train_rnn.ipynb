{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    " import torch\n",
    " import torch.nn as nn\n",
    " import torch.utils.data as data\n",
    " from names_loader import NameData\n",
    " from model import RNN\n",
    " from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 3 Introduction\n",
    "So far, we've worked on an image classification task on Norb dataset and a semantic segmentation task on PASCAL VOC 2007 dataset. In today's lab, we'll build on the known concepts to construct a Recurrent Neural Network (RNN). The problem we'll try to solve is a toy problem: given the last name of a person, predict the country of origin!\n",
    "\n",
    "For this, we have provided a dataset in `data/` directory. The idea is to build an RNN that sees one __letter__ at a time and when all the letters are seen, we ask it to predict the country of origin of the name. We have, like last lab, three files to write: the model, the dataloader, and this training file. \n",
    "\n",
    "### Setting the dataloader\n",
    "We have written a dataset class for you (if you want to go back early) in `names_loader.py`. You may choose to write your own dataset class if you wish. Let's try to create two dataloader objects, one for training and one for testing. Once that is done, have a look into what the dataloader produces. You'll find the input of size `(batch_size, 18, 57)`. 18 is the maximum length of the names in the dataset. If a name is shorter than 18 letters, it is left padded. 57 is the number of characters in the alphabet. The last dimension is a one-hot vector of the character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# Initializing the dataset objects\n",
    "dataset_train = NameData('./data', 'train')\n",
    "dataset_val = NameData('./data', 'val')\n",
    "\n",
    "# Initializing the dataloader object. \n",
    "dataloader_train = data.DataLoader(\n",
    "                dataset_train, batch_size = 8, \n",
    "                shuffle = True, num_workers = 4)\n",
    "\n",
    "dataloader_val = data.DataLoader(\n",
    "                dataset_val, batch_size = 1, \n",
    "                shuffle = False, num_workers = 1)\n",
    "\n",
    "print(dataset_train.n_categories)\n",
    "# Let's test the network's outputs\n",
    "name = 'krizhevsky'\n",
    "encoding = dataset_train.nameToTensor(name)\n",
    "print(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1])\n",
      "torch.Size([8, 1])\n",
      "torch.Size([8, 1])\n",
      "torch.Size([8, 1])\n",
      "torch.Size([8, 1])\n",
      "torch.Size([8, 1])\n",
      "torch.Size([8, 1])\n",
      "torch.Size([8, 1])\n",
      "torch.Size([8, 1])\n",
      "torch.Size([8, 1])\n",
      "torch.Size([8, 1])\n",
      "torch.Size([8, 1])\n"
     ]
    }
   ],
   "source": [
    "# Let's investigate the output of the dataloader\n",
    "for i, (input, target) in enumerate(dataloader_train):\n",
    "    print(target.size())\n",
    "    if i > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the model, criterion and the optimizer\n",
    "Let's try to load the network now. This should be routine by now! Also convert the model and criterion into cuda variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (i2h): Linear(in_features=1081, out_features=1024, bias=True)\n",
      "  (h2o): Linear(in_features=1024, out_features=18, bias=True)\n",
      "  (i2o): Linear(in_features=1081, out_features=18, bias=True)\n",
      "  (softmax): LogSoftmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Initialize the network. Hidden size: 1024.\n",
    "# 57 is the length of the one-hot-encoded input at each timestep\n",
    "model = RNN(57, 1024, dataset_train.n_categories)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Convert model and criterion into cuda here\n",
    "model.cuda()\n",
    "criterion.cuda()\n",
    "\n",
    "# Print the RNN\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__One Question that no one* asked:__ Where's the softmax happening? We never wrote a softmax layer in our neural network. Then how come we're still managing with CrossEntropy Loss?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__One more question that no one asked:__ Why are the labels not one hot encoded for cross entropy loss?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also initialize an optimizer for the task. You're free to make your hyperparameter decisions in this regard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), 0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're ready to train our first RNN. But before we do that, we need to write the train function that iterates over the data, forward props, computes the losses, backprops and  finally updates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, dataloader, model, criterion, optimizer, categories, split = 'train'):\n",
    "    # Useful for some book-keeping \n",
    "    loss_meter, acc_meter, count = 0, 0, 0\n",
    "\n",
    "    # Call model.eval if we're doing validation \n",
    "    if split == 'valid' or split == 'test':\n",
    "         model = model.eval()\n",
    "\n",
    "    for i, (input, target) in enumerate(dataloader):\n",
    "        input = Variable(input.float()).cuda()\n",
    "        target = Variable(target.reshape(-1,)).long().cuda()\n",
    "    \n",
    "        # Initializing the hidden state\n",
    "        batch_size = input.size(0)\n",
    "        hidden = Variable(model.init_hidden(batch_size)).cuda()\n",
    "\n",
    "        # seq_len = input.size(1)\n",
    "        model.zero_grad()\n",
    "        \n",
    "        for f in range(input.size(1)):\n",
    "            output, hidden = model(input[:,f,:], hidden)\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "        acc = accuracy(output, target)\n",
    "        loss_meter += loss.data.cpu().numpy()\n",
    "        acc_meter += acc\n",
    "\n",
    "        if split == 'train':\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # A must-do step to avoid the exploding gradient problem.\n",
    "            # We're restricting the norm of the the gradients to less than 5.\n",
    "            # The effects of this may not be visible in this toy problem, but\n",
    "            # can be seen when dealing with more complicated problems.\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "            optimizer.step()\n",
    "\n",
    "        count += 1\n",
    "        # print('loss at epoch ', str(epoch), ' iteration ', str(i), ' is: ', loss.data.cpu().numpy())\n",
    "        if i % 500 == 0:\n",
    "            print(split + ' epoch ', epoch, ' iteration ', i, ' loss is : ', \n",
    "                  loss_meter / count, ' accuracy is  ', acc_meter / count)\n",
    "\n",
    "    print(split + ' loss at epoch ', str(epoch), ' is: ', loss_meter / count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also write a funciton `accuracy` that computes the accuracy of our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    " def accuracy(pred, gt):\n",
    "     pred = pred.argmax(1)\n",
    "     correct, count = 0, 0\n",
    "     for i in range(pred.size(0)):\n",
    "         if pred[i] == gt[i]:\n",
    "             correct += 1\n",
    "         count += 1\n",
    "     accuracy = correct / count\n",
    "     return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now is the time to enter the training loop. We'll iterate for `n_epoch` times and validate after every second epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch  0  iteration  0  loss is :  2.8879857063293457  accuracy is   0.0\n",
      "train epoch  0  iteration  500  loss is :  2.0999727074257626  accuracy is   0.4540918163672655\n",
      "train epoch  0  iteration  1000  loss is :  1.947241179890685  accuracy is   0.4621628371628372\n",
      "train epoch  0  iteration  1500  loss is :  1.8824079184632234  accuracy is   0.4624417055296469\n",
      "train loss at epoch  0  is:  1.846264190308253\n",
      "train epoch  1  iteration  0  loss is :  1.81686270236969  accuracy is   0.5\n",
      "train epoch  1  iteration  500  loss is :  1.6388645449322379  accuracy is   0.48577844311377244\n",
      "train epoch  1  iteration  1000  loss is :  1.647268255928775  accuracy is   0.47977022977022976\n",
      "train epoch  1  iteration  1500  loss is :  1.6392047827558307  accuracy is   0.4814290473017988\n",
      "train loss at epoch  1  is:  1.631337331787745\n",
      "***************** Validation Loop Starts *********************\n",
      "val epoch  1  iteration  0  loss is :  0.365123987197876  accuracy is   1.0\n",
      "val epoch  1  iteration  500  loss is :  1.550273962839397  accuracy is   0.48502994011976047\n",
      "val epoch  1  iteration  1000  loss is :  1.5671827201243047  accuracy is   0.4745254745254745\n",
      "val epoch  1  iteration  1500  loss is :  1.5953110705845202  accuracy is   0.4643570952698201\n",
      "val epoch  1  iteration  2000  loss is :  1.5755898060648517  accuracy is   0.47476261869065467\n",
      "val epoch  1  iteration  2500  loss is :  1.5815041316885416  accuracy is   0.4778088764494202\n",
      "val epoch  1  iteration  3000  loss is :  1.5865612441561532  accuracy is   0.47750749750083304\n",
      "val epoch  1  iteration  3500  loss is :  1.5798116088151863  accuracy is   0.48014852899171667\n",
      "val epoch  1  iteration  4000  loss is :  1.5722445405980583  accuracy is   0.4823794051487128\n",
      "val epoch  1  iteration  4500  loss is :  1.575955955459393  accuracy is   0.48122639413463675\n",
      "val epoch  1  iteration  5000  loss is :  1.5729181871154838  accuracy is   0.4833033393321336\n",
      "val loss at epoch  1  is:  1.5717071017083757\n",
      "***************** Validation Loop Ends *********************\n",
      "train epoch  2  iteration  0  loss is :  1.201462984085083  accuracy is   0.625\n",
      "train epoch  2  iteration  500  loss is :  1.5530973575310316  accuracy is   0.5154690618762475\n",
      "train epoch  2  iteration  1000  loss is :  1.5317206048108005  accuracy is   0.5253496503496503\n",
      "train epoch  2  iteration  1500  loss is :  1.521918211794948  accuracy is   0.5328947368421053\n",
      "train loss at epoch  2  is:  1.5053844361464181\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 3\n",
    "categories = dataset_train.all_categories\n",
    "\n",
    "for i in range(n_epoch):\n",
    "    train(i, dataloader_train, model, criterion, optimizer, categories, 'train')\n",
    "    if i % 2 == 1:\n",
    "     print('***************** Validation Loop Starts *********************')\n",
    "     train(i, dataloader_val, model, criterion, optimizer, categories, 'val')\n",
    "     print('***************** Validation Loop Ends *********************')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Last thing to do for the workshop\n",
    "Let's test the model now. This is simple. Think of any name for testing, encode it as per the RNN's requirements and forward prop it. Let's do that now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# Let's test the network's outputs\n",
    "name = 'Smith'\n",
    "encoding = dataset_train.nameToTensor(name)\n",
    "print(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([18, 57])\n"
     ]
    }
   ],
   "source": [
    "print(encoding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = encoding.reshape(1,18,57)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3435,  0.5734, -0.8832,  2.1969, -0.3696, -1.4263,  1.1668, -0.3840,\n",
      "          2.3474, -1.2357, -0.4289,  1.4868, -1.3425, -0.3231,  0.5616, -1.1998,\n",
      "          0.3409, -0.6710]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor(8, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Should probably have wrapped this in a function, but here's the dirty forward pass through the RNN\n",
    "input = Variable(encoding.float()).cuda()\n",
    "\n",
    "# Initializing the hidden state\n",
    "batch_size = input.size(0)\n",
    "hidden = Variable(model.init_hidden(batch_size)).cuda()\n",
    "\n",
    "for f in range(input.size(1)):\n",
    "    output, hidden = model(input[:,f,:], hidden)\n",
    "\n",
    "print(output)\n",
    "print(output.argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Spanish', 'German', 'Polish', 'Russian', 'Chinese', 'Portuguese', 'Japanese', 'French', 'English', 'Korean', 'Irish', 'Arabic', 'Vietnamese', 'Dutch', 'Italian', 'Scottish', 'Czech', 'Greek']\n",
      "0 Spanish\n",
      "1 German\n",
      "2 Polish\n",
      "3 Russian\n",
      "4 Chinese\n",
      "5 Portuguese\n",
      "6 Japanese\n",
      "7 French\n",
      "8 English\n",
      "9 Korean\n",
      "10 Irish\n",
      "11 Arabic\n",
      "12 Vietnamese\n",
      "13 Dutch\n",
      "14 Italian\n",
      "15 Scottish\n",
      "16 Czech\n",
      "17 Greek\n"
     ]
    }
   ],
   "source": [
    "# Let's see what are the language indices\n",
    "for i in range(len(dataset_train.all_categories)):\n",
    "    print(i, dataset_train.all_categories[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
