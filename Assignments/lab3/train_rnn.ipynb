{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " import torch\n",
    " import torch.nn as nn\n",
    " import torch.utils.data as data\n",
    " from names_loader import NameData\n",
    " from model import RNN\n",
    " from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 3 Introduction\n",
    "So far, we've worked on an image classification task on Norb dataset and a semantic segmentation task on PASCAL VOC 2007 dataset. In today's lab, we'll build on the known concepts to construct a Recurrent Neural Network (RNN). The problem we'll try to solve is a toy problem: given the last name of a person, predict the country of origin!\n",
    "\n",
    "For this, we have provided a dataset in `data/` directory. The idea is to build an RNN that sees one __letter__ at a time and when all the letters are seen, we ask it to predict the country of origin of the name. We have, like last lab, three files to write: the model, the dataloader, and this training file. \n",
    "\n",
    "### Setting the dataloader\n",
    "We have written a dataset class for you (if you want to go back early) in `names_loader.py`. You may choose to write your own dataset class if you wish. Let's try to create two dataloader objects, one for training and one for testing. Once that is done, have a look into what the dataloader produces. You'll find the input of size `(batch_size, 18, 57)`. 18 is the maximum length of the names in the dataset. If a name is shorter than 18 letters, it is left padded. 57 is the number of characters in the alphabet. The last dimension is a one-hot vector of the character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "# Initializing the dataset objects\n",
    "dataset_train = NameData('./data', 'train')\n",
    "dataset_val = NameData('./data', 'val')\n",
    "\n",
    "# Initializing the dataloader object. \n",
    "dataloader_train = data.DataLoader(\n",
    "                dataset_train, batch_size = 8, \n",
    "                shuffle = True, num_workers = 4)\n",
    "\n",
    "dataloader_val = data.DataLoader(\n",
    "                dataset_val, batch_size = 1, \n",
    "                shuffle = False, num_workers = 4)\n",
    "\n",
    "print(dataset_train.n_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 18, 57])\n",
      "torch.Size([8, 18, 57])\n",
      "torch.Size([8, 18, 57])\n",
      "torch.Size([8, 18, 57])\n",
      "torch.Size([8, 18, 57])\n",
      "torch.Size([8, 18, 57])\n",
      "torch.Size([8, 18, 57])\n",
      "torch.Size([8, 18, 57])\n",
      "torch.Size([8, 18, 57])\n",
      "torch.Size([8, 18, 57])\n",
      "torch.Size([8, 18, 57])\n",
      "torch.Size([8, 18, 57])\n"
     ]
    }
   ],
   "source": [
    "# Let's investigate the output of the dataloader\n",
    "for i, (input, target) in enumerate(dataloader_train):\n",
    "    print(input.size())\n",
    "    if i > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the model, criterion and the optimizer\n",
    "Let's try to load the network now. This should be routine by now! Also convert the model and criterion into cuda variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (i2h): Linear(in_features=1081, out_features=1024, bias=True)\n",
      "  (i2o): Linear(in_features=1081, out_features=18, bias=True)\n",
      "  (softmax): LogSoftmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Initialize the network. Hidden size: 1024.\n",
    "# 57 is the length of the one-hot-encoded input at each timestep\n",
    "model = RNN(57, 1024, dataset_train.n_categories)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# Convert model and criterion into cuda here\n",
    "model.cuda()\n",
    "criterion.cuda()\n",
    "\n",
    "# Print the RNN\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also initialize an optimizer for the task. You're free to make your hyperparameter decisions in this regard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), 0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're ready to train our first RNN. But before we do that, we need to write the train function that iterates over the data, forward props, computes the losses, backprops and  finally updates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch, dataloader, model, criterion, optimizer, categories, split = 'train'):\n",
    "    # Useful for some book-keeping \n",
    "    loss_meter, acc_meter, count = 0, 0, 0\n",
    "\n",
    "    # Call model.eval if we're doing validation \n",
    "    if split == 'valid' or split == 'test':\n",
    "         model = model.eval()\n",
    "\n",
    "    for i, (input, target) in enumerate(dataloader):\n",
    "        input = Variable(input.float()).cuda()\n",
    "        target = Variable(target.reshape(-1,)).long().cuda()\n",
    "    \n",
    "        # Initializing the hidden state\n",
    "        batch_size = input.size(0)\n",
    "        hidden = Variable(model.init_hidden(batch_size)).cuda()\n",
    "\n",
    "        # seq_len = input.size(1)\n",
    "        model.zero_grad()\n",
    "        \n",
    "        for f in range(input.size(1)):\n",
    "            output, hidden = model(input[:,f,:], hidden)\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "        acc = accuracy(output, target)\n",
    "        loss_meter += loss.data.cpu().numpy()\n",
    "        acc_meter += acc\n",
    "\n",
    "        if split == 'train':\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # A must-do step to avoid the exploding gradient problem.\n",
    "            # We're restricting the norm of the the gradients to less than 5.\n",
    "            # The effects of this may not be visible in this toy problem, but\n",
    "            # can be seen when dealing with more complicated problems.\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "            optimizer.step()\n",
    "\n",
    "        count += 1\n",
    "        # print('loss at epoch ', str(epoch), ' iteration ', str(i), ' is: ', loss.data.cpu().numpy())\n",
    "        if i % 500 == 0:\n",
    "            print(split + ' epoch ', epoch, ' iteration ', i, ' loss is : ', \n",
    "                  loss_meter / count, ' accuracy is  ', acc_meter / count)\n",
    "\n",
    "    print(split + ' loss at epoch ', str(epoch), ' is: ', loss_meter / count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also write a funciton `accuracy` that computes the accuracy of our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " def accuracy(pred, gt):\n",
    "     pred = pred.argmax(1)\n",
    "     correct, count = 0, 0\n",
    "     for i in range(pred.size(0)):\n",
    "         if pred[i] == gt[i]:\n",
    "             correct += 1\n",
    "         count += 1\n",
    "     accuracy = correct / count\n",
    "     return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now is the time to enter the training loop. We'll iterate for `n_epoch` times and validate after every second epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/anurag/WareHouse/datasets/basics2apps/Omitted_Assignment/lab3/model.py:19: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output = self.softmax(output)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch  0  iteration  0  loss is :  2.907675266265869  accuracy is   0.0\n",
      "train epoch  0  iteration  500  loss is :  2.0793430933695354  accuracy is   0.4593313373253493\n",
      "train epoch  0  iteration  1000  loss is :  1.9381074958390647  accuracy is   0.466033966033966\n",
      "train epoch  0  iteration  1500  loss is :  1.8790211497665166  accuracy is   0.46085942704863425\n",
      "train loss at epoch  0  is:  1.8377416158517201\n",
      "train epoch  1  iteration  0  loss is :  1.4721016883850098  accuracy is   0.625\n",
      "train epoch  1  iteration  500  loss is :  1.6903273558426284  accuracy is   0.46457085828343314\n",
      "train epoch  1  iteration  1000  loss is :  1.664608210057288  accuracy is   0.47115384615384615\n",
      "train epoch  1  iteration  1500  loss is :  1.6444629468296783  accuracy is   0.47759826782145237\n",
      "train loss at epoch  1  is:  1.6243074260870616\n",
      "***************** Validation Loop *********************\n",
      "val epoch  1  iteration  0  loss is :  0.2869863510131836  accuracy is   1.0\n",
      "val epoch  1  iteration  500  loss is :  1.5369239300786854  accuracy is   0.46706586826347307\n",
      "val epoch  1  iteration  1000  loss is :  1.539854773036488  accuracy is   0.47352647352647353\n",
      "val epoch  1  iteration  1500  loss is :  1.5571864482802125  accuracy is   0.4703530979347102\n",
      "val epoch  1  iteration  2000  loss is :  1.545347328247993  accuracy is   0.48075962018990503\n",
      "val epoch  1  iteration  2500  loss is :  1.5363287595880837  accuracy is   0.4842063174730108\n",
      "val epoch  1  iteration  3000  loss is :  1.5448789297442005  accuracy is   0.4805064978340553\n",
      "val epoch  1  iteration  3500  loss is :  1.538796200509822  accuracy is   0.48129105969722935\n",
      "val epoch  1  iteration  4000  loss is :  1.5408314602639013  accuracy is   0.4816295926018495\n",
      "val epoch  1  iteration  4500  loss is :  1.5454498163675419  accuracy is   0.4807820484336814\n",
      "val epoch  1  iteration  5000  loss is :  1.5292017682746182  accuracy is   0.4859028194361128\n",
      "val loss at epoch  1  is:  1.5306500393572782\n",
      "train epoch  2  iteration  0  loss is :  2.43886661529541  accuracy is   0.25\n",
      "train epoch  2  iteration  500  loss is :  1.5520541570381727  accuracy is   0.5167165668662674\n",
      "train epoch  2  iteration  1000  loss is :  1.514772089092167  accuracy is   0.5374625374625375\n",
      "train epoch  2  iteration  1500  loss is :  1.4914766860794497  accuracy is   0.5481345769487008\n",
      "train loss at epoch  2  is:  1.478452082379659\n",
      "train epoch  3  iteration  0  loss is :  1.888928771018982  accuracy is   0.5\n",
      "train epoch  3  iteration  500  loss is :  1.4218575712330566  accuracy is   0.5855788423153693\n",
      "train epoch  3  iteration  1000  loss is :  1.388216362043575  accuracy is   0.597027972027972\n",
      "train epoch  3  iteration  1500  loss is :  1.379574359694296  accuracy is   0.6031812125249834\n",
      "train loss at epoch  3  is:  1.366599923992157\n",
      "***************** Validation Loop *********************\n",
      "val epoch  3  iteration  0  loss is :  0.13801336288452148  accuracy is   1.0\n",
      "val epoch  3  iteration  500  loss is :  1.2951267105852535  accuracy is   0.6167664670658682\n",
      "val epoch  3  iteration  1000  loss is :  1.3063049702258496  accuracy is   0.6183816183816184\n",
      "val epoch  3  iteration  1500  loss is :  1.325805891521131  accuracy is   0.603597601598934\n",
      "val epoch  3  iteration  2000  loss is :  1.3203756806851625  accuracy is   0.6036981509245377\n",
      "val epoch  3  iteration  2500  loss is :  1.3182326345527615  accuracy is   0.6041583366653339\n",
      "val epoch  3  iteration  3000  loss is :  1.3254429579019467  accuracy is   0.6007997334221926\n",
      "val epoch  3  iteration  3500  loss is :  1.3242493738483885  accuracy is   0.6006855184233076\n",
      "val epoch  3  iteration  4000  loss is :  1.3271277362601335  accuracy is   0.6000999750062485\n",
      "val epoch  3  iteration  4500  loss is :  1.330411352758486  accuracy is   0.5996445234392357\n",
      "val epoch  3  iteration  5000  loss is :  1.3116750577000993  accuracy is   0.6056788642271546\n",
      "val loss at epoch  3  is:  1.3128176784487189\n",
      "train epoch  4  iteration  0  loss is :  2.2721385955810547  accuracy is   0.25\n",
      "train epoch  4  iteration  500  loss is :  1.320506598897085  accuracy is   0.6237524950099801\n",
      "train epoch  4  iteration  1000  loss is :  1.29896402338168  accuracy is   0.6333666333666333\n",
      "train epoch  4  iteration  1500  loss is :  1.2879575872167122  accuracy is   0.6367421718854097\n",
      "train loss at epoch  4  is:  1.285877508131663\n",
      "train epoch  5  iteration  0  loss is :  1.7569602727890015  accuracy is   0.625\n",
      "train epoch  5  iteration  500  loss is :  1.2608864368078951  accuracy is   0.6472055888223552\n",
      "train epoch  5  iteration  1000  loss is :  1.2334578061377728  accuracy is   0.653971028971029\n",
      "train epoch  5  iteration  1500  loss is :  1.2287287305665762  accuracy is   0.6527315123251166\n",
      "train loss at epoch  5  is:  1.2184986957550048\n",
      "***************** Validation Loop *********************\n",
      "val epoch  5  iteration  0  loss is :  0.13065624237060547  accuracy is   1.0\n",
      "val epoch  5  iteration  500  loss is :  1.0966591482866785  accuracy is   0.7105788423153693\n",
      "val epoch  5  iteration  1000  loss is :  1.1348506277734107  accuracy is   0.7042957042957043\n",
      "val epoch  5  iteration  1500  loss is :  1.152464624963388  accuracy is   0.6882078614257162\n",
      "val epoch  5  iteration  2000  loss is :  1.160118991169317  accuracy is   0.6821589205397302\n",
      "val epoch  5  iteration  2500  loss is :  1.1545986928066603  accuracy is   0.6825269892043183\n",
      "val epoch  5  iteration  3000  loss is :  1.164226564634883  accuracy is   0.6767744085304899\n",
      "val epoch  5  iteration  3500  loss is :  1.1594236811989547  accuracy is   0.6795201371036846\n",
      "val epoch  5  iteration  4000  loss is :  1.1626778559159172  accuracy is   0.6773306673331667\n",
      "val epoch  5  iteration  4500  loss is :  1.1706779056113445  accuracy is   0.6751832926016441\n",
      "val epoch  5  iteration  5000  loss is :  1.1528485990290498  accuracy is   0.6808638272345531\n",
      "val loss at epoch  5  is:  1.1545209724883372\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 6\n",
    "categories = dataset_train.all_categories\n",
    "\n",
    "for i in range(n_epoch):\n",
    "    train(i, dataloader_train, model, criterion, optimizer, categories, 'train')\n",
    "    if i % 2 == 1:\n",
    "     print('***************** Validation Loop *********************')\n",
    "     train(i, dataloader_val, model, criterion, optimizer, categories, 'val')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
